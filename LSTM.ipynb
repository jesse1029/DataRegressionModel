{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from keras.layers import concatenate\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import jieba, pdb\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value\n",
    "    \n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "\n",
    "def word2vec(x):\n",
    "    words = jieba.cut(str(x), cut_all=False)\n",
    "    vec = np.zeros((100))\n",
    "    cnt = 1\n",
    "\n",
    "    for word in words:\n",
    "        if (word not in stopword_set) and (word != ' ') and (word in model.wv.vocab):\n",
    "            vec += model[word]\n",
    "            cnt +=1\n",
    "    vec /= cnt\n",
    "    return vec\n",
    "\n",
    "def Convert_orderid(x):\n",
    "    return str(x).strip('\\n')\n",
    "\n",
    "def cv(x):\n",
    "    x=x.replace('\\n','')\n",
    "    xx=x.split(' ')\n",
    "    y=np.zeros((64))\n",
    "    for i in range(64):          \n",
    "        print(xx[i+1])\n",
    "        y[i]=float(xx[i+1])          \n",
    "    return y\n",
    "\n",
    "def Convert_Date(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = pd.to_datetime(Year+'-'+Month+'-'+Day)\n",
    "    return date1\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in sen:\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train_lstm(p_n_symbols, p_embedding_weights, p_X_train, p_y_train, p_X_test, p_y_test):\n",
    "    print('Creating the multi-model LSTM model...')\n",
    "    early_stopping=keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    verbose=0, \n",
    "    mode='auto'\n",
    ")\n",
    "    lr=keras.callbacks.ReduceLROnPlateau(\n",
    " monitor='val_loss', \n",
    " factor=0.1, \n",
    " patience=10, \n",
    " verbose=0, \n",
    " mode='auto', \n",
    " epsilon=0.0001, \n",
    " cooldown=0, \n",
    " min_lr=0\n",
    ")\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(output_dim=140+14, input_dim=p_n_symbols, input_length=140+14))\n",
    "    model.add(LSTM(200, activation='sigmoid', name='lstm1'))\n",
    "    model.add(Dense(64, name='dense-1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(32, name='dense0'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(16, name='dense1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1, name='dense2'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print(\"Start to train a model...\")\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[auc_roc, 'accuracy'])\n",
    "    model.fit(p_X_train, p_y_train, batch_size=256, nb_epoch=30,callbacks=[lr],\n",
    "          shuffle=True, validation_data=(p_X_test, p_y_test))\n",
    "    return model\n",
    "\n",
    "#     score, acc = model.evaluate(p_X_test, p_y_test, batch_size=batch_size)\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n",
    "\n",
    "n_symbols = len(index_dict) + 1  # 索引数字的个数，因为有的词语索引为0，所以+1\n",
    "embedding_weights = np.zeros((n_symbols, 100+15))  # 创建l一个n_symbols * 100的0矩阵\n",
    "for w, index in index_dict.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "    embedding_weights[index, 15:] = word_vectors[w]  # 词向量矩阵，第一行是0向量（没有索引为0的词语，未被填充）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_order = pd.read_csv(\"dataset/order.csv\")\n",
    "df_group = pd.read_csv(\"dataset/group.csv\")\n",
    "df_airline = pd.read_csv(\"dataset/airline.csv\")\n",
    "df_day_schedule = pd.read_csv(\"day_schedule_processed.txt\")\n",
    "df_train = pd.read_csv(\"training-set.csv\")\n",
    "df_test = pd.read_csv(\"val.csv\")\n",
    "df_result = pd.read_csv(\"testing-set.csv\")\n",
    "# date Conversion\n",
    "\n",
    "month = {'Jan': '01', 'Feb': '02' , 'Mar':'03' ,'Apr': '04', \n",
    "'May': '05', 'Jun': '06' , 'Jul': '07' , 'Aug':'08', \n",
    "'Sep':'09', 'Oct':'10' , 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "# group data\n",
    "df_group['Begin_Date']=df_group.begin_date.apply(lambda x: Convert_Date(x))\n",
    "df_group['Begin_Tick']=df_group.begin_date.apply(lambda x: Date2Ticks(x))\n",
    "df_group['SubLine']= df_group.sub_line.apply(lambda x: int(x[14:]))\n",
    "df_group['Area']= df_group.area.apply(lambda x: int(x[11:]))\n",
    "df_group['name']= df_group.area.apply(lambda x: len(x))\n",
    "df_group['group_id']=df_group.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_airline['group_id']=df_airline.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order['group_id']=df_order.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_day_schedule['group_id']=df_day_schedule.group_id.apply(lambda x: Convert_orderid(x))\n",
    "\n",
    "\n",
    "group_used_cols=['group_id','Begin_Date','Begin_Tick','days','Area','SubLine','price', 'name']\n",
    "df_train['order_id']=df_train.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_result['order_id']=df_result.order_id.apply(lambda x: Convert_orderid(x))\n",
    "\n",
    "df_order_1 = df_order.merge(df_group[group_used_cols], on='group_id')\n",
    "# for order data\n",
    "df_order_1['Order_Date']=df_order_1.order_date.apply(lambda x: Convert_Date(x))\n",
    "df_order_1['Order_Tick']=df_order_1.order_date.apply(lambda x: Date2Ticks(x))\n",
    "df_order_1['order_id']=df_order_1.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order_1['Source_1']= df_order_1.source_1.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Source_2']= df_order_1.source_2.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Unit']= df_order_1.unit.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Begin_Date']=pd.to_datetime(df_order_1['Begin_Date'])\n",
    "df_order_1['Order_Date']=pd.to_datetime(df_order_1['Order_Date'])\n",
    "df_order_1['PreDays']=(df_order_1['Begin_Date']-df_order_1['Order_Date']).dt.days\n",
    "df_order_1['Begin_Date_Weekday']= df_order_1['Begin_Date'].dt.dayofweek\n",
    "df_order_1['Order_Date_Weekday']= df_order_1['Order_Date'].dt.dayofweek\n",
    "df_order_1['Return_Date_Weekday']= (df_order_1['Begin_Date'].dt.dayofweek+df_order_1['days'])%7\n",
    "df_order_1['tick_diff'] = (df_order_1['Begin_Tick'] - df_order_1['Order_Tick'])/10000\n",
    "df_order_1['price'] = df_order_1['price']/1000\n",
    "\n",
    "order_used_columns=['order_id', 'group_id','tick_diff', 'Source_1', 'Source_2', 'Unit',\n",
    "'people_amount', 'days', 'Area', 'SubLine', 'price',\n",
    "'PreDays','Begin_Date_Weekday', 'Order_Date_Weekday', 'Return_Date_Weekday', 'name']\n",
    "\n",
    "df_order_2=df_order_1[order_used_columns].merge(df_day_schedule[['group_id','title']], on='group_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201634, 154)\n"
     ]
    }
   ],
   "source": [
    "# train/test data\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "\n",
    "Y=df_train_1['deal_or_not'].values.tolist()\n",
    "swX_tmp = df_train_1['title'].values.tolist()\n",
    "Xid = df_train_1['order_id'].values.tolist()\n",
    "del df_train_1['deal_or_not'] \n",
    "del df_train_1['title']\n",
    "del df_train_1['group_id'] \n",
    "del df_train_1['order_id']\n",
    "X = df_train_1.values.tolist()\n",
    "\n",
    "rid = df_result_1['order_id'].values.tolist()\n",
    "swrx = df_result_1['title'].values.tolist()\n",
    "del df_result_1['deal_or_not']\n",
    "del df_result_1['title']\n",
    "del df_result_1['order_id']\n",
    "del df_result_1['group_id']\n",
    "\n",
    "rx = df_result_1.values.tolist()\n",
    "\n",
    "\n",
    "sX, sY, Xid =np.asarray(X), np.asarray(Y), np.asarray(Xid)\n",
    "rx,rid = np.asarray(rx), np.asarray(rid)\n",
    "X,Y, swX=[],[], []\n",
    "for i in range(len(sY)):\n",
    "    if (int(Xid[i])<=204000):\n",
    "        X.append(sX[i,:])\n",
    "        Y.append(sY[i])\n",
    "        swX.append(swX_tmp[i])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in str(sen):\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "\n",
    "\n",
    "wX = text_to_index_array(index_dict, swX)\n",
    "wrx = text_to_index_array(index_dict, swrx)\n",
    "wX = sequence.pad_sequences(wX, maxlen=140)\n",
    "wrx = sequence.pad_sequences(wrx, maxlen=140)\n",
    "\n",
    "\n",
    "X=np.concatenate([X, wX], axis=1)\n",
    "rx=np.concatenate([rx, wrx], axis=1)\n",
    "\n",
    "    \n",
    "print(X.shape)\n",
    "\n",
    "# np.save(\"data.npy\", [X,Y,rx])\n",
    "# [X,Y,rx] = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index: [     0      2      3 ... 201630 201631 201633] ,Val Index: [     1     16     29 ... 201603 201620 201632]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 155s 852us/step - loss: 0.6040 - auc_roc: 0.5025 - acc: 0.7077 - val_loss: 0.6011 - val_auc_roc: 0.5146 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.6005 - auc_roc: 0.5237 - acc: 0.7094 - val_loss: 0.5998 - val_auc_roc: 0.5304 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.5992 - auc_roc: 0.5351 - acc: 0.7094 - val_loss: 0.5993 - val_auc_roc: 0.5389 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5986 - auc_roc: 0.5417 - acc: 0.7094 - val_loss: 0.5990 - val_auc_roc: 0.5439 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5981 - auc_roc: 0.5458 - acc: 0.7094 - val_loss: 0.5987 - val_auc_roc: 0.5472 - val_acc: 0.7094\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5979 - auc_roc: 0.5485 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5498 - val_acc: 0.7094\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5975 - auc_roc: 0.5508 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5516 - val_acc: 0.7094\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5972 - auc_roc: 0.5525 - acc: 0.7094 - val_loss: 0.5995 - val_auc_roc: 0.5532 - val_acc: 0.7095\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5970 - auc_roc: 0.5539 - acc: 0.7095 - val_loss: 0.5983 - val_auc_roc: 0.5545 - val_acc: 0.7095\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5966 - auc_roc: 0.5551 - acc: 0.7095 - val_loss: 0.5982 - val_auc_roc: 0.5557 - val_acc: 0.7095\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5964 - auc_roc: 0.5563 - acc: 0.7095 - val_loss: 0.5981 - val_auc_roc: 0.5567 - val_acc: 0.7095\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5960 - auc_roc: 0.5572 - acc: 0.7095 - val_loss: 0.5980 - val_auc_roc: 0.5577 - val_acc: 0.7095\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5958 - auc_roc: 0.5582 - acc: 0.7096 - val_loss: 0.5977 - val_auc_roc: 0.5586 - val_acc: 0.7095\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 151s 833us/step - loss: 0.5957 - auc_roc: 0.5590 - acc: 0.7096 - val_loss: 0.5975 - val_auc_roc: 0.5594 - val_acc: 0.7094\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 154s 850us/step - loss: 0.5954 - auc_roc: 0.5598 - acc: 0.7095 - val_loss: 0.5975 - val_auc_roc: 0.5601 - val_acc: 0.7094\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 169s 931us/step - loss: 0.5952 - auc_roc: 0.5605 - acc: 0.7096 - val_loss: 0.5984 - val_auc_roc: 0.5608 - val_acc: 0.7093\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 185s 1ms/step - loss: 0.5937 - auc_roc: 0.5612 - acc: 0.7096 - val_loss: 0.5925 - val_auc_roc: 0.5619 - val_acc: 0.7098\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 181s 996us/step - loss: 0.5967 - auc_roc: 0.5631 - acc: 0.7155 - val_loss: 0.5984 - val_auc_roc: 0.5636 - val_acc: 0.7094\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5775 - auc_roc: 0.5646 - acc: 0.7279 - val_loss: 0.5611 - val_auc_roc: 0.5663 - val_acc: 0.7408\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5578 - auc_roc: 0.5686 - acc: 0.7393 - val_loss: 0.5519 - val_auc_roc: 0.5708 - val_acc: 0.7426\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 154s 849us/step - loss: 0.5457 - auc_roc: 0.5735 - acc: 0.7421 - val_loss: 0.5501 - val_auc_roc: 0.5760 - val_acc: 0.7422\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.5413 - auc_roc: 0.5786 - acc: 0.7420 - val_loss: 0.5430 - val_auc_roc: 0.5811 - val_acc: 0.7423\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5396 - auc_roc: 0.5836 - acc: 0.7428 - val_loss: 0.5418 - val_auc_roc: 0.5859 - val_acc: 0.7421\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 154s 848us/step - loss: 0.5370 - auc_roc: 0.5882 - acc: 0.7433 - val_loss: 0.5416 - val_auc_roc: 0.5904 - val_acc: 0.7428\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 153s 846us/step - loss: 0.5358 - auc_roc: 0.5926 - acc: 0.7436 - val_loss: 0.5399 - val_auc_roc: 0.5947 - val_acc: 0.7418\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5341 - auc_roc: 0.5967 - acc: 0.7441 - val_loss: 0.5466 - val_auc_roc: 0.5986 - val_acc: 0.7406\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5328 - auc_roc: 0.6005 - acc: 0.7438 - val_loss: 0.5385 - val_auc_roc: 0.6024 - val_acc: 0.7426\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 154s 847us/step - loss: 0.5319 - auc_roc: 0.6042 - acc: 0.7448 - val_loss: 0.5381 - val_auc_roc: 0.6059 - val_acc: 0.7429\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5311 - auc_roc: 0.6077 - acc: 0.7447 - val_loss: 0.5379 - val_auc_roc: 0.6093 - val_acc: 0.7426\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 155s 854us/step - loss: 0.5297 - auc_roc: 0.6110 - acc: 0.7451 - val_loss: 0.5387 - val_auc_roc: 0.6125 - val_acc: 0.7431\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 156s 857us/step - loss: 0.5287 - auc_roc: 0.6141 - acc: 0.7451 - val_loss: 0.5386 - val_auc_roc: 0.6155 - val_acc: 0.7435\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 155s 854us/step - loss: 0.5278 - auc_roc: 0.6170 - acc: 0.7453 - val_loss: 0.5379 - val_auc_roc: 0.6184 - val_acc: 0.7435\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 155s 854us/step - loss: 0.5266 - auc_roc: 0.6198 - acc: 0.7462 - val_loss: 0.5389 - val_auc_roc: 0.6211 - val_acc: 0.7426\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 155s 853us/step - loss: 0.5260 - auc_roc: 0.6225 - acc: 0.7463 - val_loss: 0.5383 - val_auc_roc: 0.6237 - val_acc: 0.7435\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5247 - auc_roc: 0.6251 - acc: 0.7463 - val_loss: 0.5374 - val_auc_roc: 0.6263 - val_acc: 0.7436\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5237 - auc_roc: 0.6275 - acc: 0.7468 - val_loss: 0.5408 - val_auc_roc: 0.6287 - val_acc: 0.7431\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5234 - auc_roc: 0.6299 - acc: 0.7469 - val_loss: 0.5392 - val_auc_roc: 0.6309 - val_acc: 0.7404\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5219 - auc_roc: 0.6321 - acc: 0.7475 - val_loss: 0.5374 - val_auc_roc: 0.6332 - val_acc: 0.7430\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5209 - auc_roc: 0.6343 - acc: 0.7478 - val_loss: 0.5384 - val_auc_roc: 0.6353 - val_acc: 0.7436\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5200 - auc_roc: 0.6364 - acc: 0.7484 - val_loss: 0.5392 - val_auc_roc: 0.6374 - val_acc: 0.7431\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5186 - auc_roc: 0.6384 - acc: 0.7481 - val_loss: 0.5405 - val_auc_roc: 0.6394 - val_acc: 0.7402\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5184 - auc_roc: 0.6404 - acc: 0.7482 - val_loss: 0.5394 - val_auc_roc: 0.6413 - val_acc: 0.7405\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5168 - auc_roc: 0.6423 - acc: 0.7486 - val_loss: 0.5399 - val_auc_roc: 0.6432 - val_acc: 0.7417\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5151 - auc_roc: 0.6442 - acc: 0.7495 - val_loss: 0.5420 - val_auc_roc: 0.6451 - val_acc: 0.7406\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 154s 851us/step - loss: 0.5143 - auc_roc: 0.6460 - acc: 0.7497 - val_loss: 0.5404 - val_auc_roc: 0.6468 - val_acc: 0.7428\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 155s 854us/step - loss: 0.5062 - auc_roc: 0.6478 - acc: 0.7527 - val_loss: 0.5450 - val_auc_roc: 0.6488 - val_acc: 0.7398\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 155s 853us/step - loss: 0.5046 - auc_roc: 0.6498 - acc: 0.7533 - val_loss: 0.5460 - val_auc_roc: 0.6508 - val_acc: 0.7392\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 155s 855us/step - loss: 0.5038 - auc_roc: 0.6517 - acc: 0.7537 - val_loss: 0.5477 - val_auc_roc: 0.6527 - val_acc: 0.7387\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5030 - auc_roc: 0.6536 - acc: 0.7540 - val_loss: 0.5472 - val_auc_roc: 0.6545 - val_acc: 0.7401\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 155s 852us/step - loss: 0.5025 - auc_roc: 0.6554 - acc: 0.7543 - val_loss: 0.5491 - val_auc_roc: 0.6563 - val_acc: 0.7418\n",
      "Fold  1 AUC-LSTM : 0.683352\n",
      "Train Index: [     0      1      2 ... 201630 201631 201632] ,Val Index: [    21     55     60 ... 201609 201622 201633]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 157s 863us/step - loss: 0.6040 - auc_roc: 0.5066 - acc: 0.7075 - val_loss: 0.6004 - val_auc_roc: 0.5151 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.6002 - auc_roc: 0.5251 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5315 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5990 - auc_roc: 0.5357 - acc: 0.7094 - val_loss: 0.5996 - val_auc_roc: 0.5392 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 154s 850us/step - loss: 0.5984 - auc_roc: 0.5419 - acc: 0.7094 - val_loss: 0.5984 - val_auc_roc: 0.5441 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 154s 847us/step - loss: 0.5979 - auc_roc: 0.5461 - acc: 0.7094 - val_loss: 0.5984 - val_auc_roc: 0.5474 - val_acc: 0.7094\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5976 - auc_roc: 0.5487 - acc: 0.7094 - val_loss: 0.5993 - val_auc_roc: 0.5497 - val_acc: 0.7093\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5971 - auc_roc: 0.5508 - acc: 0.7095 - val_loss: 0.5988 - val_auc_roc: 0.5516 - val_acc: 0.7094\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 155s 856us/step - loss: 0.5969 - auc_roc: 0.5525 - acc: 0.7095 - val_loss: 0.5979 - val_auc_roc: 0.5531 - val_acc: 0.7094\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5966 - auc_roc: 0.5538 - acc: 0.7094 - val_loss: 0.5987 - val_auc_roc: 0.5545 - val_acc: 0.7094\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5964 - auc_roc: 0.5552 - acc: 0.7095 - val_loss: 0.5977 - val_auc_roc: 0.5557 - val_acc: 0.7094\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5961 - auc_roc: 0.5562 - acc: 0.7095 - val_loss: 0.5975 - val_auc_roc: 0.5568 - val_acc: 0.7095\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5959 - auc_roc: 0.5573 - acc: 0.7095 - val_loss: 0.5976 - val_auc_roc: 0.5578 - val_acc: 0.7094\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5955 - auc_roc: 0.5583 - acc: 0.7095 - val_loss: 0.5977 - val_auc_roc: 0.5588 - val_acc: 0.7094\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5952 - auc_roc: 0.5592 - acc: 0.7096 - val_loss: 0.5984 - val_auc_roc: 0.5596 - val_acc: 0.7094\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5950 - auc_roc: 0.5600 - acc: 0.7095 - val_loss: 0.5972 - val_auc_roc: 0.5604 - val_acc: 0.7095\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5916 - auc_roc: 0.5610 - acc: 0.7096 - val_loss: 0.5808 - val_auc_roc: 0.5623 - val_acc: 0.7162\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5562 - auc_roc: 0.5653 - acc: 0.7376 - val_loss: 0.5566 - val_auc_roc: 0.5685 - val_acc: 0.7378\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5435 - auc_roc: 0.5717 - acc: 0.7417 - val_loss: 0.5517 - val_auc_roc: 0.5750 - val_acc: 0.7370\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5384 - auc_roc: 0.5782 - acc: 0.7432 - val_loss: 0.5491 - val_auc_roc: 0.5812 - val_acc: 0.7383\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5349 - auc_roc: 0.5843 - acc: 0.7433 - val_loss: 0.5460 - val_auc_roc: 0.5871 - val_acc: 0.7382\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5329 - auc_roc: 0.5899 - acc: 0.7446 - val_loss: 0.5504 - val_auc_roc: 0.5925 - val_acc: 0.7378\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5316 - auc_roc: 0.5951 - acc: 0.7447 - val_loss: 0.5466 - val_auc_roc: 0.5975 - val_acc: 0.7400\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5300 - auc_roc: 0.5999 - acc: 0.7450 - val_loss: 0.5457 - val_auc_roc: 0.6022 - val_acc: 0.7365\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5286 - auc_roc: 0.6044 - acc: 0.7454 - val_loss: 0.5445 - val_auc_roc: 0.6065 - val_acc: 0.7392\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5275 - auc_roc: 0.6086 - acc: 0.7455 - val_loss: 0.5467 - val_auc_roc: 0.6106 - val_acc: 0.7367\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5269 - auc_roc: 0.6126 - acc: 0.7455 - val_loss: 0.5463 - val_auc_roc: 0.6144 - val_acc: 0.7391\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5254 - auc_roc: 0.6162 - acc: 0.7464 - val_loss: 0.5446 - val_auc_roc: 0.6179 - val_acc: 0.7392\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5240 - auc_roc: 0.6197 - acc: 0.7465 - val_loss: 0.5451 - val_auc_roc: 0.6213 - val_acc: 0.7377\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5233 - auc_roc: 0.6229 - acc: 0.7472 - val_loss: 0.5472 - val_auc_roc: 0.6245 - val_acc: 0.7378\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5226 - auc_roc: 0.6260 - acc: 0.7470 - val_loss: 0.5468 - val_auc_roc: 0.6274 - val_acc: 0.7395\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5211 - auc_roc: 0.6289 - acc: 0.7469 - val_loss: 0.5478 - val_auc_roc: 0.6303 - val_acc: 0.7383\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5209 - auc_roc: 0.6317 - acc: 0.7474 - val_loss: 0.5463 - val_auc_roc: 0.6330 - val_acc: 0.7386\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5199 - auc_roc: 0.6343 - acc: 0.7479 - val_loss: 0.5489 - val_auc_roc: 0.6355 - val_acc: 0.7383\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 153s 840us/step - loss: 0.5217 - auc_roc: 0.6367 - acc: 0.7484 - val_loss: 0.5459 - val_auc_roc: 0.6379 - val_acc: 0.7388\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5134 - auc_roc: 0.6392 - acc: 0.7508 - val_loss: 0.5486 - val_auc_roc: 0.6405 - val_acc: 0.7367\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5119 - auc_roc: 0.6418 - acc: 0.7514 - val_loss: 0.5499 - val_auc_roc: 0.6431 - val_acc: 0.7362\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5110 - auc_roc: 0.6443 - acc: 0.7513 - val_loss: 0.5505 - val_auc_roc: 0.6455 - val_acc: 0.7357\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5105 - auc_roc: 0.6467 - acc: 0.7517 - val_loss: 0.5508 - val_auc_roc: 0.6478 - val_acc: 0.7352\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5098 - auc_roc: 0.6489 - acc: 0.7518 - val_loss: 0.5520 - val_auc_roc: 0.6500 - val_acc: 0.7340\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5096 - auc_roc: 0.6510 - acc: 0.7518 - val_loss: 0.5524 - val_auc_roc: 0.6521 - val_acc: 0.7343\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5088 - auc_roc: 0.6531 - acc: 0.7519 - val_loss: 0.5529 - val_auc_roc: 0.6541 - val_acc: 0.7350\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5084 - auc_roc: 0.6550 - acc: 0.7522 - val_loss: 0.5523 - val_auc_roc: 0.6560 - val_acc: 0.7349\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5079 - auc_roc: 0.6569 - acc: 0.7524 - val_loss: 0.5541 - val_auc_roc: 0.6578 - val_acc: 0.7352\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5077 - auc_roc: 0.6587 - acc: 0.7524 - val_loss: 0.5548 - val_auc_roc: 0.6596 - val_acc: 0.7336\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5063 - auc_roc: 0.6604 - acc: 0.7527 - val_loss: 0.5546 - val_auc_roc: 0.6613 - val_acc: 0.7344\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5062 - auc_roc: 0.6621 - acc: 0.7527 - val_loss: 0.5547 - val_auc_roc: 0.6629 - val_acc: 0.7340\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5062 - auc_roc: 0.6637 - acc: 0.7527 - val_loss: 0.5546 - val_auc_roc: 0.6645 - val_acc: 0.7343\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5061 - auc_roc: 0.6652 - acc: 0.7529 - val_loss: 0.5548 - val_auc_roc: 0.6660 - val_acc: 0.7346\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5060 - auc_roc: 0.6667 - acc: 0.7528 - val_loss: 0.5547 - val_auc_roc: 0.6674 - val_acc: 0.7345\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5060 - auc_roc: 0.6681 - acc: 0.7529 - val_loss: 0.5547 - val_auc_roc: 0.6688 - val_acc: 0.7341\n",
      "Fold  2 AUC-LSTM : 0.671048\n",
      "Train Index: [     0      1      2 ... 201631 201632 201633] ,Val Index: [     4     17     22 ... 201569 201624 201630]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 155s 855us/step - loss: 0.6028 - auc_roc: 0.5115 - acc: 0.7094 - val_loss: 0.6037 - val_auc_roc: 0.5207 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5995 - auc_roc: 0.5294 - acc: 0.7094 - val_loss: 0.5991 - val_auc_roc: 0.5357 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5985 - auc_roc: 0.5401 - acc: 0.7094 - val_loss: 0.5990 - val_auc_roc: 0.5429 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5976 - auc_roc: 0.5456 - acc: 0.7094 - val_loss: 0.5987 - val_auc_roc: 0.5475 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5972 - auc_roc: 0.5490 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5505 - val_acc: 0.7094\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5967 - auc_roc: 0.5518 - acc: 0.7094 - val_loss: 0.5990 - val_auc_roc: 0.5527 - val_acc: 0.7094\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5961 - auc_roc: 0.5537 - acc: 0.7094 - val_loss: 0.6003 - val_auc_roc: 0.5547 - val_acc: 0.7094\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5957 - auc_roc: 0.5556 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5564 - val_acc: 0.7094\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5952 - auc_roc: 0.5573 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5580 - val_acc: 0.7094\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5949 - auc_roc: 0.5588 - acc: 0.7094 - val_loss: 0.5986 - val_auc_roc: 0.5593 - val_acc: 0.7094\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5947 - auc_roc: 0.5600 - acc: 0.7094 - val_loss: 0.5987 - val_auc_roc: 0.5605 - val_acc: 0.7094\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5943 - auc_roc: 0.5611 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5616 - val_acc: 0.7094\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5941 - auc_roc: 0.5621 - acc: 0.7094 - val_loss: 0.5982 - val_auc_roc: 0.5625 - val_acc: 0.7094\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 154s 848us/step - loss: 0.5938 - auc_roc: 0.5631 - acc: 0.7094 - val_loss: 0.5977 - val_auc_roc: 0.5635 - val_acc: 0.7094\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5937 - auc_roc: 0.5640 - acc: 0.7094 - val_loss: 0.5984 - val_auc_roc: 0.5643 - val_acc: 0.7094\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5932 - auc_roc: 0.5648 - acc: 0.7094 - val_loss: 0.5982 - val_auc_roc: 0.5651 - val_acc: 0.7094\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5930 - auc_roc: 0.5656 - acc: 0.7094 - val_loss: 0.5974 - val_auc_roc: 0.5659 - val_acc: 0.7094\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5930 - auc_roc: 0.5663 - acc: 0.7094 - val_loss: 0.5985 - val_auc_roc: 0.5666 - val_acc: 0.7094\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5924 - auc_roc: 0.5670 - acc: 0.7094 - val_loss: 0.5968 - val_auc_roc: 0.5674 - val_acc: 0.7094\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5884 - auc_roc: 0.5679 - acc: 0.7095 - val_loss: 0.5842 - val_auc_roc: 0.5690 - val_acc: 0.7116\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5682 - auc_roc: 0.5709 - acc: 0.7311 - val_loss: 0.5618 - val_auc_roc: 0.5729 - val_acc: 0.7378\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5520 - auc_roc: 0.5750 - acc: 0.7401 - val_loss: 0.5526 - val_auc_roc: 0.5772 - val_acc: 0.7387\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5438 - auc_roc: 0.5795 - acc: 0.7412 - val_loss: 0.5477 - val_auc_roc: 0.5818 - val_acc: 0.7387\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5390 - auc_roc: 0.5842 - acc: 0.7423 - val_loss: 0.5472 - val_auc_roc: 0.5864 - val_acc: 0.7382\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5355 - auc_roc: 0.5887 - acc: 0.7425 - val_loss: 0.5443 - val_auc_roc: 0.5910 - val_acc: 0.7425\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5327 - auc_roc: 0.5932 - acc: 0.7435 - val_loss: 0.5445 - val_auc_roc: 0.5953 - val_acc: 0.7401\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5311 - auc_roc: 0.5975 - acc: 0.7440 - val_loss: 0.5460 - val_auc_roc: 0.5994 - val_acc: 0.7400\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5298 - auc_roc: 0.6014 - acc: 0.7443 - val_loss: 0.5399 - val_auc_roc: 0.6033 - val_acc: 0.7416\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5285 - auc_roc: 0.6052 - acc: 0.7443 - val_loss: 0.5413 - val_auc_roc: 0.6070 - val_acc: 0.7407\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5269 - auc_roc: 0.6088 - acc: 0.7449 - val_loss: 0.5405 - val_auc_roc: 0.6105 - val_acc: 0.7414\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5255 - auc_roc: 0.6122 - acc: 0.7456 - val_loss: 0.5397 - val_auc_roc: 0.6138 - val_acc: 0.7412\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5242 - auc_roc: 0.6154 - acc: 0.7457 - val_loss: 0.5428 - val_auc_roc: 0.6169 - val_acc: 0.7416\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5231 - auc_roc: 0.6185 - acc: 0.7459 - val_loss: 0.5423 - val_auc_roc: 0.6199 - val_acc: 0.7417\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5217 - auc_roc: 0.6214 - acc: 0.7465 - val_loss: 0.5395 - val_auc_roc: 0.6228 - val_acc: 0.7412\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5207 - auc_roc: 0.6242 - acc: 0.7469 - val_loss: 0.5421 - val_auc_roc: 0.6255 - val_acc: 0.7417\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5196 - auc_roc: 0.6269 - acc: 0.7474 - val_loss: 0.5409 - val_auc_roc: 0.6282 - val_acc: 0.7405\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5185 - auc_roc: 0.6295 - acc: 0.7469 - val_loss: 0.5452 - val_auc_roc: 0.6307 - val_acc: 0.7425\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5167 - auc_roc: 0.6320 - acc: 0.7480 - val_loss: 0.5445 - val_auc_roc: 0.6332 - val_acc: 0.7405\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5158 - auc_roc: 0.6344 - acc: 0.7483 - val_loss: 0.5480 - val_auc_roc: 0.6355 - val_acc: 0.7404\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5141 - auc_roc: 0.6367 - acc: 0.7490 - val_loss: 0.5443 - val_auc_roc: 0.6378 - val_acc: 0.7412\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5128 - auc_roc: 0.6390 - acc: 0.7491 - val_loss: 0.5524 - val_auc_roc: 0.6400 - val_acc: 0.7397\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5116 - auc_roc: 0.6412 - acc: 0.7491 - val_loss: 0.5431 - val_auc_roc: 0.6422 - val_acc: 0.7404\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5100 - auc_roc: 0.6433 - acc: 0.7502 - val_loss: 0.5416 - val_auc_roc: 0.6443 - val_acc: 0.7397\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5087 - auc_roc: 0.6454 - acc: 0.7500 - val_loss: 0.5478 - val_auc_roc: 0.6463 - val_acc: 0.7385\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.4984 - auc_roc: 0.6475 - acc: 0.7532 - val_loss: 0.5533 - val_auc_roc: 0.6486 - val_acc: 0.7397\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.4958 - auc_roc: 0.6498 - acc: 0.7542 - val_loss: 0.5563 - val_auc_roc: 0.6509 - val_acc: 0.7391\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.4944 - auc_roc: 0.6520 - acc: 0.7548 - val_loss: 0.5579 - val_auc_roc: 0.6531 - val_acc: 0.7383\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.4935 - auc_roc: 0.6541 - acc: 0.7548 - val_loss: 0.5590 - val_auc_roc: 0.6552 - val_acc: 0.7387\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.4927 - auc_roc: 0.6562 - acc: 0.7552 - val_loss: 0.5599 - val_auc_roc: 0.6572 - val_acc: 0.7391\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.4919 - auc_roc: 0.6582 - acc: 0.7554 - val_loss: 0.5638 - val_auc_roc: 0.6592 - val_acc: 0.7386\n",
      "Fold  3 AUC-LSTM : 0.677931\n",
      "Train Index: [     0      1      2 ... 201631 201632 201633] ,Val Index: [     7      9     13 ... 201591 201606 201621]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 157s 863us/step - loss: 0.6028 - auc_roc: 0.5122 - acc: 0.7089 - val_loss: 0.6014 - val_auc_roc: 0.5208 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.6001 - auc_roc: 0.5282 - acc: 0.7094 - val_loss: 0.5999 - val_auc_roc: 0.5341 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5987 - auc_roc: 0.5382 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5416 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5978 - auc_roc: 0.5446 - acc: 0.7094 - val_loss: 0.6009 - val_auc_roc: 0.5466 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5972 - auc_roc: 0.5484 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5500 - val_acc: 0.7094\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5967 - auc_roc: 0.5515 - acc: 0.7094 - val_loss: 0.5985 - val_auc_roc: 0.5527 - val_acc: 0.7094\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5962 - auc_roc: 0.5539 - acc: 0.7095 - val_loss: 0.6019 - val_auc_roc: 0.5549 - val_acc: 0.7091\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5959 - auc_roc: 0.5558 - acc: 0.7095 - val_loss: 0.5982 - val_auc_roc: 0.5566 - val_acc: 0.7093\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5890 - auc_roc: 0.5591 - acc: 0.7128 - val_loss: 0.6013 - val_auc_roc: 0.5623 - val_acc: 0.7094\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5630 - auc_roc: 0.5660 - acc: 0.7329 - val_loss: 0.5964 - val_auc_roc: 0.5707 - val_acc: 0.7128\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5536 - auc_roc: 0.5749 - acc: 0.7378 - val_loss: 0.5538 - val_auc_roc: 0.5793 - val_acc: 0.7351\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5440 - auc_roc: 0.5836 - acc: 0.7410 - val_loss: 0.5507 - val_auc_roc: 0.5878 - val_acc: 0.7352\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5408 - auc_roc: 0.5918 - acc: 0.7421 - val_loss: 0.5510 - val_auc_roc: 0.5954 - val_acc: 0.7351\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5384 - auc_roc: 0.5989 - acc: 0.7424 - val_loss: 0.5439 - val_auc_roc: 0.6022 - val_acc: 0.7372\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5362 - auc_roc: 0.6053 - acc: 0.7428 - val_loss: 0.5436 - val_auc_roc: 0.6082 - val_acc: 0.7383\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5348 - auc_roc: 0.6111 - acc: 0.7436 - val_loss: 0.5448 - val_auc_roc: 0.6137 - val_acc: 0.7404\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5329 - auc_roc: 0.6162 - acc: 0.7439 - val_loss: 0.5419 - val_auc_roc: 0.6186 - val_acc: 0.7393\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5315 - auc_roc: 0.6210 - acc: 0.7439 - val_loss: 0.5419 - val_auc_roc: 0.6231 - val_acc: 0.7401\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5301 - auc_roc: 0.6253 - acc: 0.7443 - val_loss: 0.5396 - val_auc_roc: 0.6273 - val_acc: 0.7409\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5292 - auc_roc: 0.6292 - acc: 0.7446 - val_loss: 0.5408 - val_auc_roc: 0.6311 - val_acc: 0.7405\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5277 - auc_roc: 0.6329 - acc: 0.7449 - val_loss: 0.5411 - val_auc_roc: 0.6346 - val_acc: 0.7391\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5268 - auc_roc: 0.6363 - acc: 0.7456 - val_loss: 0.5393 - val_auc_roc: 0.6379 - val_acc: 0.7402\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5256 - auc_roc: 0.6394 - acc: 0.7457 - val_loss: 0.5399 - val_auc_roc: 0.6409 - val_acc: 0.7401\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5246 - auc_roc: 0.6424 - acc: 0.7463 - val_loss: 0.5388 - val_auc_roc: 0.6437 - val_acc: 0.7408\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5230 - auc_roc: 0.6451 - acc: 0.7467 - val_loss: 0.5426 - val_auc_roc: 0.6464 - val_acc: 0.7400\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5222 - auc_roc: 0.6477 - acc: 0.7471 - val_loss: 0.5414 - val_auc_roc: 0.6489 - val_acc: 0.7402\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5208 - auc_roc: 0.6502 - acc: 0.7474 - val_loss: 0.5426 - val_auc_roc: 0.6513 - val_acc: 0.7397\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5191 - auc_roc: 0.6525 - acc: 0.7484 - val_loss: 0.5487 - val_auc_roc: 0.6536 - val_acc: 0.7344\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5185 - auc_roc: 0.6548 - acc: 0.7485 - val_loss: 0.5401 - val_auc_roc: 0.6558 - val_acc: 0.7397\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5171 - auc_roc: 0.6569 - acc: 0.7489 - val_loss: 0.5418 - val_auc_roc: 0.6579 - val_acc: 0.7386\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5156 - auc_roc: 0.6590 - acc: 0.7493 - val_loss: 0.5521 - val_auc_roc: 0.6599 - val_acc: 0.7298\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5143 - auc_roc: 0.6609 - acc: 0.7502 - val_loss: 0.5441 - val_auc_roc: 0.6619 - val_acc: 0.7377\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5130 - auc_roc: 0.6628 - acc: 0.7506 - val_loss: 0.5478 - val_auc_roc: 0.6638 - val_acc: 0.7405\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5114 - auc_roc: 0.6647 - acc: 0.7509 - val_loss: 0.5447 - val_auc_roc: 0.6656 - val_acc: 0.7391\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5026 - auc_roc: 0.6666 - acc: 0.7545 - val_loss: 0.5525 - val_auc_roc: 0.6677 - val_acc: 0.7355\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5004 - auc_roc: 0.6688 - acc: 0.7553 - val_loss: 0.5548 - val_auc_roc: 0.6698 - val_acc: 0.7353\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.4993 - auc_roc: 0.6708 - acc: 0.7556 - val_loss: 0.5551 - val_auc_roc: 0.6718 - val_acc: 0.7360\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.4985 - auc_roc: 0.6727 - acc: 0.7558 - val_loss: 0.5558 - val_auc_roc: 0.6737 - val_acc: 0.7348\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.4978 - auc_roc: 0.6746 - acc: 0.7563 - val_loss: 0.5583 - val_auc_roc: 0.6755 - val_acc: 0.7318\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.4971 - auc_roc: 0.6764 - acc: 0.7566 - val_loss: 0.5570 - val_auc_roc: 0.6773 - val_acc: 0.7333\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.4965 - auc_roc: 0.6781 - acc: 0.7568 - val_loss: 0.5606 - val_auc_roc: 0.6789 - val_acc: 0.7311\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.4960 - auc_roc: 0.6797 - acc: 0.7573 - val_loss: 0.5577 - val_auc_roc: 0.6805 - val_acc: 0.7352\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.4954 - auc_roc: 0.6813 - acc: 0.7574 - val_loss: 0.5605 - val_auc_roc: 0.6821 - val_acc: 0.7347\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.4948 - auc_roc: 0.6828 - acc: 0.7574 - val_loss: 0.5600 - val_auc_roc: 0.6836 - val_acc: 0.7321\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.4930 - auc_roc: 0.6843 - acc: 0.7583 - val_loss: 0.5627 - val_auc_roc: 0.6850 - val_acc: 0.7337\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.4929 - auc_roc: 0.6857 - acc: 0.7584 - val_loss: 0.5629 - val_auc_roc: 0.6864 - val_acc: 0.7333\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.4928 - auc_roc: 0.6871 - acc: 0.7585 - val_loss: 0.5626 - val_auc_roc: 0.6877 - val_acc: 0.7336\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.4927 - auc_roc: 0.6884 - acc: 0.7586 - val_loss: 0.5637 - val_auc_roc: 0.6890 - val_acc: 0.7330\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.4926 - auc_roc: 0.6896 - acc: 0.7586 - val_loss: 0.5629 - val_auc_roc: 0.6902 - val_acc: 0.7336\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.4926 - auc_roc: 0.6908 - acc: 0.7586 - val_loss: 0.5639 - val_auc_roc: 0.6914 - val_acc: 0.7333\n",
      "Fold  4 AUC-LSTM : 0.672268\n",
      "Train Index: [     0      1      2 ... 201631 201632 201633] ,Val Index: [     6     15     18 ... 201615 201626 201627]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 156s 858us/step - loss: 0.6025 - auc_roc: 0.5052 - acc: 0.7094 - val_loss: 0.6017 - val_auc_roc: 0.5160 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.6002 - auc_roc: 0.5253 - acc: 0.7094 - val_loss: 0.5997 - val_auc_roc: 0.5317 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5989 - auc_roc: 0.5366 - acc: 0.7094 - val_loss: 0.5988 - val_auc_roc: 0.5401 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5979 - auc_roc: 0.5432 - acc: 0.7094 - val_loss: 0.5986 - val_auc_roc: 0.5453 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5972 - auc_roc: 0.5475 - acc: 0.7094 - val_loss: 0.5985 - val_auc_roc: 0.5492 - val_acc: 0.7093\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5966 - auc_roc: 0.5508 - acc: 0.7095 - val_loss: 0.5984 - val_auc_roc: 0.5522 - val_acc: 0.7093\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5942 - auc_roc: 0.5539 - acc: 0.7094 - val_loss: 0.5938 - val_auc_roc: 0.5564 - val_acc: 0.7094\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5919 - auc_roc: 0.5589 - acc: 0.7100 - val_loss: 0.5847 - val_auc_roc: 0.5608 - val_acc: 0.7302\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5622 - auc_roc: 0.5654 - acc: 0.7370 - val_loss: 0.5554 - val_auc_roc: 0.5702 - val_acc: 0.7394\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5496 - auc_roc: 0.5753 - acc: 0.7412 - val_loss: 0.5495 - val_auc_roc: 0.5800 - val_acc: 0.7411\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5443 - auc_roc: 0.5848 - acc: 0.7417 - val_loss: 0.5490 - val_auc_roc: 0.5891 - val_acc: 0.7406\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5410 - auc_roc: 0.5933 - acc: 0.7427 - val_loss: 0.5459 - val_auc_roc: 0.5970 - val_acc: 0.7406\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5380 - auc_roc: 0.6008 - acc: 0.7430 - val_loss: 0.5431 - val_auc_roc: 0.6043 - val_acc: 0.7421\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5356 - auc_roc: 0.6076 - acc: 0.7437 - val_loss: 0.5467 - val_auc_roc: 0.6108 - val_acc: 0.7383\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5337 - auc_roc: 0.6137 - acc: 0.7443 - val_loss: 0.5428 - val_auc_roc: 0.6166 - val_acc: 0.7399\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5321 - auc_roc: 0.6193 - acc: 0.7451 - val_loss: 0.5422 - val_auc_roc: 0.6218 - val_acc: 0.7424\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 153s 840us/step - loss: 0.5306 - auc_roc: 0.6243 - acc: 0.7453 - val_loss: 0.5417 - val_auc_roc: 0.6265 - val_acc: 0.7433\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5296 - auc_roc: 0.6287 - acc: 0.7453 - val_loss: 0.5419 - val_auc_roc: 0.6308 - val_acc: 0.7432\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5286 - auc_roc: 0.6328 - acc: 0.7454 - val_loss: 0.5469 - val_auc_roc: 0.6347 - val_acc: 0.7427\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5273 - auc_roc: 0.6365 - acc: 0.7462 - val_loss: 0.5406 - val_auc_roc: 0.6382 - val_acc: 0.7421\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5263 - auc_roc: 0.6400 - acc: 0.7463 - val_loss: 0.5442 - val_auc_roc: 0.6415 - val_acc: 0.7434\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5252 - auc_roc: 0.6431 - acc: 0.7467 - val_loss: 0.5402 - val_auc_roc: 0.6446 - val_acc: 0.7436\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 153s 840us/step - loss: 0.5243 - auc_roc: 0.6461 - acc: 0.7468 - val_loss: 0.5410 - val_auc_roc: 0.6474 - val_acc: 0.7431\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5235 - auc_roc: 0.6488 - acc: 0.7475 - val_loss: 0.5503 - val_auc_roc: 0.6500 - val_acc: 0.7422\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5227 - auc_roc: 0.6513 - acc: 0.7476 - val_loss: 0.5445 - val_auc_roc: 0.6525 - val_acc: 0.7429\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5219 - auc_roc: 0.6537 - acc: 0.7478 - val_loss: 0.5418 - val_auc_roc: 0.6548 - val_acc: 0.7405\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5206 - auc_roc: 0.6559 - acc: 0.7485 - val_loss: 0.5430 - val_auc_roc: 0.6570 - val_acc: 0.7400\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5202 - auc_roc: 0.6581 - acc: 0.7486 - val_loss: 0.5479 - val_auc_roc: 0.6590 - val_acc: 0.7425\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5191 - auc_roc: 0.6601 - acc: 0.7491 - val_loss: 0.5438 - val_auc_roc: 0.6610 - val_acc: 0.7422\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 156s 857us/step - loss: 0.5181 - auc_roc: 0.6620 - acc: 0.7489 - val_loss: 0.5428 - val_auc_roc: 0.6629 - val_acc: 0.7416\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5171 - auc_roc: 0.6639 - acc: 0.7488 - val_loss: 0.5467 - val_auc_roc: 0.6647 - val_acc: 0.7340\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 153s 840us/step - loss: 0.5157 - auc_roc: 0.6656 - acc: 0.7500 - val_loss: 0.5426 - val_auc_roc: 0.6665 - val_acc: 0.7409\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5086 - auc_roc: 0.6675 - acc: 0.7531 - val_loss: 0.5486 - val_auc_roc: 0.6685 - val_acc: 0.7381\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5071 - auc_roc: 0.6695 - acc: 0.7535 - val_loss: 0.5471 - val_auc_roc: 0.6705 - val_acc: 0.7386\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5062 - auc_roc: 0.6714 - acc: 0.7537 - val_loss: 0.5479 - val_auc_roc: 0.6724 - val_acc: 0.7389\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5058 - auc_roc: 0.6733 - acc: 0.7541 - val_loss: 0.5486 - val_auc_roc: 0.6741 - val_acc: 0.7392\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5053 - auc_roc: 0.6750 - acc: 0.7540 - val_loss: 0.5492 - val_auc_roc: 0.6758 - val_acc: 0.7392\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5049 - auc_roc: 0.6767 - acc: 0.7539 - val_loss: 0.5485 - val_auc_roc: 0.6775 - val_acc: 0.7385\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5045 - auc_roc: 0.6782 - acc: 0.7544 - val_loss: 0.5498 - val_auc_roc: 0.6790 - val_acc: 0.7386\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5042 - auc_roc: 0.6797 - acc: 0.7544 - val_loss: 0.5504 - val_auc_roc: 0.6805 - val_acc: 0.7379\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5038 - auc_roc: 0.6812 - acc: 0.7547 - val_loss: 0.5519 - val_auc_roc: 0.6819 - val_acc: 0.7393\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5035 - auc_roc: 0.6826 - acc: 0.7544 - val_loss: 0.5499 - val_auc_roc: 0.6832 - val_acc: 0.7390\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5022 - auc_roc: 0.6839 - acc: 0.7553 - val_loss: 0.5513 - val_auc_roc: 0.6845 - val_acc: 0.7384\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5021 - auc_roc: 0.6852 - acc: 0.7554 - val_loss: 0.5511 - val_auc_roc: 0.6858 - val_acc: 0.7379\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 151s 835us/step - loss: 0.5020 - auc_roc: 0.6864 - acc: 0.7553 - val_loss: 0.5514 - val_auc_roc: 0.6870 - val_acc: 0.7371\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5020 - auc_roc: 0.6876 - acc: 0.7553 - val_loss: 0.5514 - val_auc_roc: 0.6882 - val_acc: 0.7375\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5019 - auc_roc: 0.6887 - acc: 0.7554 - val_loss: 0.5515 - val_auc_roc: 0.6893 - val_acc: 0.7374\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 151s 834us/step - loss: 0.5019 - auc_roc: 0.6898 - acc: 0.7555 - val_loss: 0.5515 - val_auc_roc: 0.6903 - val_acc: 0.7372\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5019 - auc_roc: 0.6908 - acc: 0.7553 - val_loss: 0.5518 - val_auc_roc: 0.6913 - val_acc: 0.7371\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5018 - auc_roc: 0.6918 - acc: 0.7555 - val_loss: 0.5516 - val_auc_roc: 0.6923 - val_acc: 0.7382\n",
      "Fold  5 AUC-LSTM : 0.675073\n",
      "Train Index: [     0      1      2 ... 201631 201632 201633] ,Val Index: [    14     19     23 ... 201598 201602 201607]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 156s 859us/step - loss: 0.6024 - auc_roc: 0.5074 - acc: 0.7094 - val_loss: 0.6009 - val_auc_roc: 0.5197 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.6001 - auc_roc: 0.5282 - acc: 0.7094 - val_loss: 0.6005 - val_auc_roc: 0.5336 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5987 - auc_roc: 0.5380 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5414 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5979 - auc_roc: 0.5440 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5462 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5972 - auc_roc: 0.5481 - acc: 0.7095 - val_loss: 0.5988 - val_auc_roc: 0.5497 - val_acc: 0.7094\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5933 - auc_roc: 0.5517 - acc: 0.7098 - val_loss: 0.5816 - val_auc_roc: 0.5557 - val_acc: 0.7158\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5618 - auc_roc: 0.5636 - acc: 0.7337 - val_loss: 0.5534 - val_auc_roc: 0.5712 - val_acc: 0.7390\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5455 - auc_roc: 0.5786 - acc: 0.7407 - val_loss: 0.5502 - val_auc_roc: 0.5851 - val_acc: 0.7398\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5452 - auc_roc: 0.5908 - acc: 0.7413 - val_loss: 0.5455 - val_auc_roc: 0.5960 - val_acc: 0.7402\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5388 - auc_roc: 0.6010 - acc: 0.7425 - val_loss: 0.5432 - val_auc_roc: 0.6054 - val_acc: 0.7404\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5364 - auc_roc: 0.6096 - acc: 0.7434 - val_loss: 0.5412 - val_auc_roc: 0.6134 - val_acc: 0.7410\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5346 - auc_roc: 0.6171 - acc: 0.7440 - val_loss: 0.5404 - val_auc_roc: 0.6203 - val_acc: 0.7414\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5338 - auc_roc: 0.6234 - acc: 0.7443 - val_loss: 0.5414 - val_auc_roc: 0.6261 - val_acc: 0.7408\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 153s 840us/step - loss: 0.5328 - auc_roc: 0.6286 - acc: 0.7442 - val_loss: 0.5410 - val_auc_roc: 0.6311 - val_acc: 0.7405\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5313 - auc_roc: 0.6334 - acc: 0.7447 - val_loss: 0.5397 - val_auc_roc: 0.6356 - val_acc: 0.7413\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5304 - auc_roc: 0.6377 - acc: 0.7449 - val_loss: 0.5403 - val_auc_roc: 0.6395 - val_acc: 0.7407\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5294 - auc_roc: 0.6414 - acc: 0.7453 - val_loss: 0.5420 - val_auc_roc: 0.6431 - val_acc: 0.7396\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5285 - auc_roc: 0.6448 - acc: 0.7461 - val_loss: 0.5391 - val_auc_roc: 0.6463 - val_acc: 0.7402\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5278 - auc_roc: 0.6479 - acc: 0.7459 - val_loss: 0.5398 - val_auc_roc: 0.6492 - val_acc: 0.7405\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5266 - auc_roc: 0.6506 - acc: 0.7463 - val_loss: 0.5401 - val_auc_roc: 0.6520 - val_acc: 0.7395\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 155s 853us/step - loss: 0.5256 - auc_roc: 0.6533 - acc: 0.7468 - val_loss: 0.5399 - val_auc_roc: 0.6545 - val_acc: 0.7411\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5246 - auc_roc: 0.6557 - acc: 0.7469 - val_loss: 0.5402 - val_auc_roc: 0.6568 - val_acc: 0.7413\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5235 - auc_roc: 0.6580 - acc: 0.7476 - val_loss: 0.5404 - val_auc_roc: 0.6591 - val_acc: 0.7418\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5227 - auc_roc: 0.6601 - acc: 0.7477 - val_loss: 0.5418 - val_auc_roc: 0.6611 - val_acc: 0.7424\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5214 - auc_roc: 0.6622 - acc: 0.7484 - val_loss: 0.5419 - val_auc_roc: 0.6631 - val_acc: 0.7408\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5203 - auc_roc: 0.6641 - acc: 0.7490 - val_loss: 0.5483 - val_auc_roc: 0.6650 - val_acc: 0.7340\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5195 - auc_roc: 0.6659 - acc: 0.7490 - val_loss: 0.5447 - val_auc_roc: 0.6667 - val_acc: 0.7416\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5178 - auc_roc: 0.6677 - acc: 0.7496 - val_loss: 0.5438 - val_auc_roc: 0.6685 - val_acc: 0.7411\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 153s 840us/step - loss: 0.5111 - auc_roc: 0.6695 - acc: 0.7521 - val_loss: 0.5449 - val_auc_roc: 0.6704 - val_acc: 0.7387\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5096 - auc_roc: 0.6714 - acc: 0.7527 - val_loss: 0.5462 - val_auc_roc: 0.6724 - val_acc: 0.7382\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5089 - auc_roc: 0.6733 - acc: 0.7527 - val_loss: 0.5459 - val_auc_roc: 0.6742 - val_acc: 0.7380\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5083 - auc_roc: 0.6751 - acc: 0.7533 - val_loss: 0.5471 - val_auc_roc: 0.6759 - val_acc: 0.7378\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5079 - auc_roc: 0.6768 - acc: 0.7532 - val_loss: 0.5487 - val_auc_roc: 0.6775 - val_acc: 0.7378\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5075 - auc_roc: 0.6783 - acc: 0.7532 - val_loss: 0.5474 - val_auc_roc: 0.6791 - val_acc: 0.7382\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5071 - auc_roc: 0.6798 - acc: 0.7537 - val_loss: 0.5469 - val_auc_roc: 0.6805 - val_acc: 0.7384\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 152s 838us/step - loss: 0.5066 - auc_roc: 0.6812 - acc: 0.7538 - val_loss: 0.5475 - val_auc_roc: 0.6819 - val_acc: 0.7375\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5063 - auc_roc: 0.6826 - acc: 0.7537 - val_loss: 0.5479 - val_auc_roc: 0.6833 - val_acc: 0.7390\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5060 - auc_roc: 0.6839 - acc: 0.7539 - val_loss: 0.5503 - val_auc_roc: 0.6845 - val_acc: 0.7387\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5046 - auc_roc: 0.6851 - acc: 0.7545 - val_loss: 0.5503 - val_auc_roc: 0.6857 - val_acc: 0.7378\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5045 - auc_roc: 0.6863 - acc: 0.7545 - val_loss: 0.5503 - val_auc_roc: 0.6869 - val_acc: 0.7374\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 153s 842us/step - loss: 0.5045 - auc_roc: 0.6875 - acc: 0.7544 - val_loss: 0.5507 - val_auc_roc: 0.6880 - val_acc: 0.7375\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 155s 853us/step - loss: 0.5044 - auc_roc: 0.6886 - acc: 0.7545 - val_loss: 0.5507 - val_auc_roc: 0.6891 - val_acc: 0.7374\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 156s 858us/step - loss: 0.5044 - auc_roc: 0.6896 - acc: 0.7545 - val_loss: 0.5507 - val_auc_roc: 0.6901 - val_acc: 0.7376\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 155s 856us/step - loss: 0.5043 - auc_roc: 0.6906 - acc: 0.7546 - val_loss: 0.5510 - val_auc_roc: 0.6911 - val_acc: 0.7372\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 155s 852us/step - loss: 0.5043 - auc_roc: 0.6915 - acc: 0.7545 - val_loss: 0.5510 - val_auc_roc: 0.6920 - val_acc: 0.7375\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 155s 852us/step - loss: 0.5042 - auc_roc: 0.6924 - acc: 0.7545 - val_loss: 0.5514 - val_auc_roc: 0.6929 - val_acc: 0.7375\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 154s 850us/step - loss: 0.5042 - auc_roc: 0.6933 - acc: 0.7546 - val_loss: 0.5513 - val_auc_roc: 0.6937 - val_acc: 0.7375\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 155s 853us/step - loss: 0.5042 - auc_roc: 0.6941 - acc: 0.7546 - val_loss: 0.5513 - val_auc_roc: 0.6945 - val_acc: 0.7376\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 156s 860us/step - loss: 0.5040 - auc_roc: 0.6949 - acc: 0.7546 - val_loss: 0.5513 - val_auc_roc: 0.6953 - val_acc: 0.7376\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.5040 - auc_roc: 0.6957 - acc: 0.7547 - val_loss: 0.5513 - val_auc_roc: 0.6960 - val_acc: 0.7376\n",
      "Fold  6 AUC-LSTM : 0.676805\n",
      "Train Index: [     1      2      3 ... 201631 201632 201633] ,Val Index: [     0     27     33 ... 201588 201618 201628]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181470 samples, validate on 20164 samples\n",
      "Epoch 1/50\n",
      "181470/181470 [==============================] - 157s 864us/step - loss: 0.6025 - auc_roc: 0.5080 - acc: 0.7094 - val_loss: 0.6009 - val_auc_roc: 0.5218 - val_acc: 0.7094\n",
      "Epoch 2/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5998 - auc_roc: 0.5293 - acc: 0.7094 - val_loss: 0.6037 - val_auc_roc: 0.5349 - val_acc: 0.7094\n",
      "Epoch 3/50\n",
      "181470/181470 [==============================] - 154s 846us/step - loss: 0.5986 - auc_roc: 0.5390 - acc: 0.7095 - val_loss: 0.5996 - val_auc_roc: 0.5419 - val_acc: 0.7094\n",
      "Epoch 4/50\n",
      "181470/181470 [==============================] - 152s 840us/step - loss: 0.5976 - auc_roc: 0.5449 - acc: 0.7095 - val_loss: 0.5998 - val_auc_roc: 0.5469 - val_acc: 0.7094\n",
      "Epoch 5/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.5973 - auc_roc: 0.5488 - acc: 0.7095 - val_loss: 0.5998 - val_auc_roc: 0.5501 - val_acc: 0.7094\n",
      "Epoch 6/50\n",
      "181470/181470 [==============================] - 153s 842us/step - loss: 0.5966 - auc_roc: 0.5516 - acc: 0.7095 - val_loss: 0.6016 - val_auc_roc: 0.5526 - val_acc: 0.7094\n",
      "Epoch 7/50\n",
      "181470/181470 [==============================] - 157s 864us/step - loss: 0.5963 - auc_roc: 0.5537 - acc: 0.7094 - val_loss: 0.5992 - val_auc_roc: 0.5546 - val_acc: 0.7094\n",
      "Epoch 8/50\n",
      "181470/181470 [==============================] - 153s 841us/step - loss: 0.5958 - auc_roc: 0.5555 - acc: 0.7095 - val_loss: 0.6002 - val_auc_roc: 0.5564 - val_acc: 0.7094\n",
      "Epoch 9/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.5956 - auc_roc: 0.5573 - acc: 0.7095 - val_loss: 0.5996 - val_auc_roc: 0.5579 - val_acc: 0.7094\n",
      "Epoch 10/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5952 - auc_roc: 0.5587 - acc: 0.7095 - val_loss: 0.5995 - val_auc_roc: 0.5593 - val_acc: 0.7094\n",
      "Epoch 11/50\n",
      "181470/181470 [==============================] - 154s 847us/step - loss: 0.5949 - auc_roc: 0.5600 - acc: 0.7095 - val_loss: 0.5991 - val_auc_roc: 0.5604 - val_acc: 0.7091\n",
      "Epoch 12/50\n",
      "181470/181470 [==============================] - 154s 846us/step - loss: 0.5947 - auc_roc: 0.5610 - acc: 0.7095 - val_loss: 0.5984 - val_auc_roc: 0.5615 - val_acc: 0.7091\n",
      "Epoch 13/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5939 - auc_roc: 0.5621 - acc: 0.7095 - val_loss: 0.5989 - val_auc_roc: 0.5627 - val_acc: 0.7089\n",
      "Epoch 14/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5922 - auc_roc: 0.5634 - acc: 0.7095 - val_loss: 0.5897 - val_auc_roc: 0.5644 - val_acc: 0.7091\n",
      "Epoch 15/50\n",
      "181470/181470 [==============================] - 154s 848us/step - loss: 0.5687 - auc_roc: 0.5673 - acc: 0.7260 - val_loss: 0.5536 - val_auc_roc: 0.5705 - val_acc: 0.7366\n",
      "Epoch 16/50\n",
      "181470/181470 [==============================] - 154s 849us/step - loss: 0.5445 - auc_roc: 0.5742 - acc: 0.7405 - val_loss: 0.5469 - val_auc_roc: 0.5778 - val_acc: 0.7400\n",
      "Epoch 17/50\n",
      "181470/181470 [==============================] - 154s 851us/step - loss: 0.5390 - auc_roc: 0.5813 - acc: 0.7422 - val_loss: 0.5439 - val_auc_roc: 0.5847 - val_acc: 0.7400\n",
      "Epoch 18/50\n",
      "181470/181470 [==============================] - 154s 849us/step - loss: 0.5356 - auc_roc: 0.5879 - acc: 0.7430 - val_loss: 0.5435 - val_auc_roc: 0.5910 - val_acc: 0.7398\n",
      "Epoch 19/50\n",
      "181470/181470 [==============================] - 154s 847us/step - loss: 0.5343 - auc_roc: 0.5940 - acc: 0.7434 - val_loss: 0.5405 - val_auc_roc: 0.5968 - val_acc: 0.7409\n",
      "Epoch 20/50\n",
      "181470/181470 [==============================] - 154s 850us/step - loss: 0.5317 - auc_roc: 0.5996 - acc: 0.7442 - val_loss: 0.5429 - val_auc_roc: 0.6022 - val_acc: 0.7396\n",
      "Epoch 21/50\n",
      "181470/181470 [==============================] - 154s 848us/step - loss: 0.5311 - auc_roc: 0.6047 - acc: 0.7446 - val_loss: 0.5427 - val_auc_roc: 0.6071 - val_acc: 0.7413\n",
      "Epoch 22/50\n",
      "181470/181470 [==============================] - 154s 849us/step - loss: 0.5289 - auc_roc: 0.6094 - acc: 0.7456 - val_loss: 0.5411 - val_auc_roc: 0.6116 - val_acc: 0.7422\n",
      "Epoch 23/50\n",
      "181470/181470 [==============================] - 154s 847us/step - loss: 0.5279 - auc_roc: 0.6138 - acc: 0.7456 - val_loss: 0.5443 - val_auc_roc: 0.6158 - val_acc: 0.7406\n",
      "Epoch 24/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5267 - auc_roc: 0.6178 - acc: 0.7464 - val_loss: 0.5410 - val_auc_roc: 0.6197 - val_acc: 0.7417\n",
      "Epoch 25/50\n",
      "181470/181470 [==============================] - 153s 843us/step - loss: 0.5251 - auc_roc: 0.6216 - acc: 0.7468 - val_loss: 0.5439 - val_auc_roc: 0.6234 - val_acc: 0.7383\n",
      "Epoch 26/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5239 - auc_roc: 0.6252 - acc: 0.7468 - val_loss: 0.5411 - val_auc_roc: 0.6269 - val_acc: 0.7404\n",
      "Epoch 27/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5230 - auc_roc: 0.6286 - acc: 0.7475 - val_loss: 0.5436 - val_auc_roc: 0.6301 - val_acc: 0.7396\n",
      "Epoch 28/50\n",
      "181470/181470 [==============================] - 153s 844us/step - loss: 0.5217 - auc_roc: 0.6317 - acc: 0.7481 - val_loss: 0.5427 - val_auc_roc: 0.6332 - val_acc: 0.7392\n",
      "Epoch 29/50\n",
      "181470/181470 [==============================] - 153s 845us/step - loss: 0.5213 - auc_roc: 0.6347 - acc: 0.7479 - val_loss: 0.5433 - val_auc_roc: 0.6360 - val_acc: 0.7401\n",
      "Epoch 30/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5135 - auc_roc: 0.6376 - acc: 0.7511 - val_loss: 0.5454 - val_auc_roc: 0.6391 - val_acc: 0.7392\n",
      "Epoch 31/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5121 - auc_roc: 0.6406 - acc: 0.7515 - val_loss: 0.5486 - val_auc_roc: 0.6421 - val_acc: 0.7367\n",
      "Epoch 32/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5113 - auc_roc: 0.6435 - acc: 0.7518 - val_loss: 0.5471 - val_auc_roc: 0.6449 - val_acc: 0.7381\n",
      "Epoch 33/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5108 - auc_roc: 0.6462 - acc: 0.7521 - val_loss: 0.5475 - val_auc_roc: 0.6475 - val_acc: 0.7391\n",
      "Epoch 34/50\n",
      "181470/181470 [==============================] - 152s 835us/step - loss: 0.5102 - auc_roc: 0.6488 - acc: 0.7521 - val_loss: 0.5472 - val_auc_roc: 0.6500 - val_acc: 0.7391\n",
      "Epoch 35/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5098 - auc_roc: 0.6512 - acc: 0.7523 - val_loss: 0.5478 - val_auc_roc: 0.6523 - val_acc: 0.7376\n",
      "Epoch 36/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5093 - auc_roc: 0.6535 - acc: 0.7522 - val_loss: 0.5503 - val_auc_roc: 0.6546 - val_acc: 0.7355\n",
      "Epoch 37/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5089 - auc_roc: 0.6556 - acc: 0.7524 - val_loss: 0.5491 - val_auc_roc: 0.6567 - val_acc: 0.7386\n",
      "Epoch 38/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5085 - auc_roc: 0.6577 - acc: 0.7526 - val_loss: 0.5496 - val_auc_roc: 0.6587 - val_acc: 0.7380\n",
      "Epoch 39/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5081 - auc_roc: 0.6596 - acc: 0.7530 - val_loss: 0.5503 - val_auc_roc: 0.6605 - val_acc: 0.7372\n",
      "Epoch 40/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5067 - auc_roc: 0.6615 - acc: 0.7534 - val_loss: 0.5515 - val_auc_roc: 0.6624 - val_acc: 0.7367\n",
      "Epoch 41/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5066 - auc_roc: 0.6633 - acc: 0.7533 - val_loss: 0.5512 - val_auc_roc: 0.6641 - val_acc: 0.7372\n",
      "Epoch 42/50\n",
      "181470/181470 [==============================] - 152s 839us/step - loss: 0.5065 - auc_roc: 0.6650 - acc: 0.7533 - val_loss: 0.5515 - val_auc_roc: 0.6658 - val_acc: 0.7364\n",
      "Epoch 43/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5065 - auc_roc: 0.6666 - acc: 0.7534 - val_loss: 0.5518 - val_auc_roc: 0.6674 - val_acc: 0.7365\n",
      "Epoch 44/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5064 - auc_roc: 0.6681 - acc: 0.7533 - val_loss: 0.5518 - val_auc_roc: 0.6689 - val_acc: 0.7365\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5064 - auc_roc: 0.6696 - acc: 0.7535 - val_loss: 0.5517 - val_auc_roc: 0.6703 - val_acc: 0.7367\n",
      "Epoch 46/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5063 - auc_roc: 0.6710 - acc: 0.7535 - val_loss: 0.5516 - val_auc_roc: 0.6717 - val_acc: 0.7369\n",
      "Epoch 47/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5062 - auc_roc: 0.6724 - acc: 0.7535 - val_loss: 0.5521 - val_auc_roc: 0.6730 - val_acc: 0.7357\n",
      "Epoch 48/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5062 - auc_roc: 0.6737 - acc: 0.7537 - val_loss: 0.5517 - val_auc_roc: 0.6743 - val_acc: 0.7373\n",
      "Epoch 49/50\n",
      "181470/181470 [==============================] - 152s 836us/step - loss: 0.5062 - auc_roc: 0.6749 - acc: 0.7536 - val_loss: 0.5518 - val_auc_roc: 0.6755 - val_acc: 0.7371\n",
      "Epoch 50/50\n",
      "181470/181470 [==============================] - 152s 837us/step - loss: 0.5060 - auc_roc: 0.6761 - acc: 0.7535 - val_loss: 0.5519 - val_auc_roc: 0.6766 - val_acc: 0.7368\n",
      "Fold  7 AUC-LSTM : 0.673232\n",
      "Train Index: [     0      1      2 ... 201630 201632 201633] ,Val Index: [    10     12     39 ... 201619 201625 201631]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181472 samples, validate on 20162 samples\n",
      "Epoch 1/50\n",
      "181472/181472 [==============================] - 156s 859us/step - loss: 0.6030 - auc_roc: 0.5092 - acc: 0.7083 - val_loss: 0.6007 - val_auc_roc: 0.5214 - val_acc: 0.7095\n",
      "Epoch 2/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.6002 - auc_roc: 0.5290 - acc: 0.7094 - val_loss: 0.6007 - val_auc_roc: 0.5339 - val_acc: 0.7095\n",
      "Epoch 3/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5987 - auc_roc: 0.5382 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5414 - val_acc: 0.7095\n",
      "Epoch 4/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5980 - auc_roc: 0.5441 - acc: 0.7094 - val_loss: 0.5993 - val_auc_roc: 0.5462 - val_acc: 0.7095\n",
      "Epoch 5/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5973 - auc_roc: 0.5481 - acc: 0.7094 - val_loss: 0.5993 - val_auc_roc: 0.5495 - val_acc: 0.7095\n",
      "Epoch 6/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5966 - auc_roc: 0.5510 - acc: 0.7094 - val_loss: 0.5987 - val_auc_roc: 0.5523 - val_acc: 0.7095\n",
      "Epoch 7/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5957 - auc_roc: 0.5537 - acc: 0.7094 - val_loss: 0.5969 - val_auc_roc: 0.5551 - val_acc: 0.7095\n",
      "Epoch 8/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5699 - auc_roc: 0.5599 - acc: 0.7270 - val_loss: 0.5555 - val_auc_roc: 0.5660 - val_acc: 0.7389\n",
      "Epoch 9/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5496 - auc_roc: 0.5722 - acc: 0.7394 - val_loss: 0.5511 - val_auc_roc: 0.5778 - val_acc: 0.7405\n",
      "Epoch 10/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5447 - auc_roc: 0.5831 - acc: 0.7409 - val_loss: 0.5473 - val_auc_roc: 0.5880 - val_acc: 0.7401\n",
      "Epoch 11/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5426 - auc_roc: 0.5926 - acc: 0.7413 - val_loss: 0.5469 - val_auc_roc: 0.5967 - val_acc: 0.7417\n",
      "Epoch 12/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5387 - auc_roc: 0.6009 - acc: 0.7422 - val_loss: 0.5423 - val_auc_roc: 0.6045 - val_acc: 0.7427\n",
      "Epoch 13/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5368 - auc_roc: 0.6079 - acc: 0.7428 - val_loss: 0.5413 - val_auc_roc: 0.6112 - val_acc: 0.7429\n",
      "Epoch 14/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5352 - auc_roc: 0.6142 - acc: 0.7434 - val_loss: 0.5410 - val_auc_roc: 0.6171 - val_acc: 0.7414\n",
      "Epoch 15/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5333 - auc_roc: 0.6199 - acc: 0.7439 - val_loss: 0.5403 - val_auc_roc: 0.6224 - val_acc: 0.7429\n",
      "Epoch 16/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5322 - auc_roc: 0.6248 - acc: 0.7444 - val_loss: 0.5394 - val_auc_roc: 0.6271 - val_acc: 0.7434\n",
      "Epoch 17/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5306 - auc_roc: 0.6293 - acc: 0.7450 - val_loss: 0.5426 - val_auc_roc: 0.6313 - val_acc: 0.7407\n",
      "Epoch 18/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5292 - auc_roc: 0.6334 - acc: 0.7457 - val_loss: 0.5421 - val_auc_roc: 0.6352 - val_acc: 0.7396\n",
      "Epoch 19/50\n",
      "181472/181472 [==============================] - 156s 859us/step - loss: 0.5278 - auc_roc: 0.6371 - acc: 0.7453 - val_loss: 0.5386 - val_auc_roc: 0.6388 - val_acc: 0.7426\n",
      "Epoch 20/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5266 - auc_roc: 0.6405 - acc: 0.7460 - val_loss: 0.5377 - val_auc_roc: 0.6421 - val_acc: 0.7443\n",
      "Epoch 21/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5253 - auc_roc: 0.6437 - acc: 0.7468 - val_loss: 0.5405 - val_auc_roc: 0.6452 - val_acc: 0.7419\n",
      "Epoch 22/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5244 - auc_roc: 0.6467 - acc: 0.7473 - val_loss: 0.5378 - val_auc_roc: 0.6480 - val_acc: 0.7430\n",
      "Epoch 23/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5232 - auc_roc: 0.6494 - acc: 0.7472 - val_loss: 0.5386 - val_auc_roc: 0.6507 - val_acc: 0.7425\n",
      "Epoch 24/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5223 - auc_roc: 0.6520 - acc: 0.7477 - val_loss: 0.5410 - val_auc_roc: 0.6532 - val_acc: 0.7404\n",
      "Epoch 25/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5208 - auc_roc: 0.6544 - acc: 0.7481 - val_loss: 0.5398 - val_auc_roc: 0.6556 - val_acc: 0.7405\n",
      "Epoch 26/50\n",
      "181472/181472 [==============================] - 153s 843us/step - loss: 0.5203 - auc_roc: 0.6567 - acc: 0.7486 - val_loss: 0.5394 - val_auc_roc: 0.6578 - val_acc: 0.7431\n",
      "Epoch 27/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5188 - auc_roc: 0.6589 - acc: 0.7490 - val_loss: 0.5421 - val_auc_roc: 0.6599 - val_acc: 0.7410\n",
      "Epoch 28/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5176 - auc_roc: 0.6610 - acc: 0.7495 - val_loss: 0.5406 - val_auc_roc: 0.6620 - val_acc: 0.7411\n",
      "Epoch 29/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5167 - auc_roc: 0.6630 - acc: 0.7499 - val_loss: 0.5418 - val_auc_roc: 0.6639 - val_acc: 0.7414\n",
      "Epoch 30/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5151 - auc_roc: 0.6649 - acc: 0.7505 - val_loss: 0.5407 - val_auc_roc: 0.6658 - val_acc: 0.7424\n",
      "Epoch 31/50\n",
      "181472/181472 [==============================] - 153s 845us/step - loss: 0.5071 - auc_roc: 0.6669 - acc: 0.7534 - val_loss: 0.5458 - val_auc_roc: 0.6679 - val_acc: 0.7401\n",
      "Epoch 32/50\n",
      "181472/181472 [==============================] - 153s 846us/step - loss: 0.5053 - auc_roc: 0.6690 - acc: 0.7541 - val_loss: 0.5456 - val_auc_roc: 0.6700 - val_acc: 0.7396\n",
      "Epoch 33/50\n",
      "181472/181472 [==============================] - 153s 845us/step - loss: 0.5046 - auc_roc: 0.6710 - acc: 0.7544 - val_loss: 0.5467 - val_auc_roc: 0.6719 - val_acc: 0.7405\n",
      "Epoch 34/50\n",
      "181472/181472 [==============================] - 153s 842us/step - loss: 0.5038 - auc_roc: 0.6729 - acc: 0.7548 - val_loss: 0.5483 - val_auc_roc: 0.6738 - val_acc: 0.7391\n",
      "Epoch 35/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5033 - auc_roc: 0.6747 - acc: 0.7549 - val_loss: 0.5458 - val_auc_roc: 0.6756 - val_acc: 0.7415\n",
      "Epoch 36/50\n",
      "181472/181472 [==============================] - 153s 842us/step - loss: 0.5027 - auc_roc: 0.6764 - acc: 0.7549 - val_loss: 0.5482 - val_auc_roc: 0.6772 - val_acc: 0.7390\n",
      "Epoch 37/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5024 - auc_roc: 0.6780 - acc: 0.7553 - val_loss: 0.5483 - val_auc_roc: 0.6788 - val_acc: 0.7386\n",
      "Epoch 38/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5018 - auc_roc: 0.6796 - acc: 0.7555 - val_loss: 0.5491 - val_auc_roc: 0.6803 - val_acc: 0.7388\n",
      "Epoch 39/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5014 - auc_roc: 0.6811 - acc: 0.7556 - val_loss: 0.5502 - val_auc_roc: 0.6818 - val_acc: 0.7393\n",
      "Epoch 40/50\n",
      "181472/181472 [==============================] - 153s 840us/step - loss: 0.5024 - auc_roc: 0.6825 - acc: 0.7553 - val_loss: 0.5508 - val_auc_roc: 0.6831 - val_acc: 0.7394\n",
      "Epoch 41/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.4999 - auc_roc: 0.6838 - acc: 0.7562 - val_loss: 0.5505 - val_auc_roc: 0.6845 - val_acc: 0.7386\n",
      "Epoch 42/50\n",
      "181472/181472 [==============================] - 153s 843us/step - loss: 0.4998 - auc_roc: 0.6851 - acc: 0.7563 - val_loss: 0.5509 - val_auc_roc: 0.6857 - val_acc: 0.7386\n",
      "Epoch 43/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.4997 - auc_roc: 0.6863 - acc: 0.7562 - val_loss: 0.5513 - val_auc_roc: 0.6870 - val_acc: 0.7382\n",
      "Epoch 44/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.4996 - auc_roc: 0.6875 - acc: 0.7563 - val_loss: 0.5512 - val_auc_roc: 0.6881 - val_acc: 0.7385\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181472/181472 [==============================] - 153s 842us/step - loss: 0.4995 - auc_roc: 0.6887 - acc: 0.7563 - val_loss: 0.5512 - val_auc_roc: 0.6892 - val_acc: 0.7382\n",
      "Epoch 46/50\n",
      "181472/181472 [==============================] - 153s 842us/step - loss: 0.4994 - auc_roc: 0.6898 - acc: 0.7564 - val_loss: 0.5515 - val_auc_roc: 0.6903 - val_acc: 0.7381\n",
      "Epoch 47/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.4993 - auc_roc: 0.6908 - acc: 0.7563 - val_loss: 0.5517 - val_auc_roc: 0.6913 - val_acc: 0.7380\n",
      "Epoch 48/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.4992 - auc_roc: 0.6918 - acc: 0.7563 - val_loss: 0.5515 - val_auc_roc: 0.6923 - val_acc: 0.7381\n",
      "Epoch 49/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.4991 - auc_roc: 0.6927 - acc: 0.7564 - val_loss: 0.5518 - val_auc_roc: 0.6932 - val_acc: 0.7379\n",
      "Epoch 50/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.4990 - auc_roc: 0.6937 - acc: 0.7565 - val_loss: 0.5516 - val_auc_roc: 0.6941 - val_acc: 0.7389\n",
      "Fold  8 AUC-LSTM : 0.682941\n",
      "Train Index: [     0      1      3 ... 201631 201632 201633] ,Val Index: [     2      8     11 ... 201593 201610 201614]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181472 samples, validate on 20162 samples\n",
      "Epoch 1/50\n",
      "181472/181472 [==============================] - 156s 860us/step - loss: 0.6023 - auc_roc: 0.5083 - acc: 0.7094 - val_loss: 0.6012 - val_auc_roc: 0.5228 - val_acc: 0.7095\n",
      "Epoch 2/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.6000 - auc_roc: 0.5296 - acc: 0.7094 - val_loss: 0.6005 - val_auc_roc: 0.5351 - val_acc: 0.7095\n",
      "Epoch 3/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5987 - auc_roc: 0.5390 - acc: 0.7094 - val_loss: 0.6004 - val_auc_roc: 0.5420 - val_acc: 0.7095\n",
      "Epoch 4/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5979 - auc_roc: 0.5441 - acc: 0.7094 - val_loss: 0.5997 - val_auc_roc: 0.5463 - val_acc: 0.7095\n",
      "Epoch 5/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5973 - auc_roc: 0.5480 - acc: 0.7094 - val_loss: 0.6011 - val_auc_roc: 0.5494 - val_acc: 0.7095\n",
      "Epoch 6/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5968 - auc_roc: 0.5506 - acc: 0.7094 - val_loss: 0.6002 - val_auc_roc: 0.5519 - val_acc: 0.7095\n",
      "Epoch 7/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5939 - auc_roc: 0.5535 - acc: 0.7094 - val_loss: 0.5848 - val_auc_roc: 0.5564 - val_acc: 0.7105\n",
      "Epoch 8/50\n",
      "181472/181472 [==============================] - 153s 840us/step - loss: 0.5927 - auc_roc: 0.5582 - acc: 0.7099 - val_loss: 0.5808 - val_auc_roc: 0.5608 - val_acc: 0.7124\n",
      "Epoch 9/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5639 - auc_roc: 0.5661 - acc: 0.7344 - val_loss: 0.5556 - val_auc_roc: 0.5713 - val_acc: 0.7422\n",
      "Epoch 10/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5497 - auc_roc: 0.5766 - acc: 0.7407 - val_loss: 0.5465 - val_auc_roc: 0.5816 - val_acc: 0.7424\n",
      "Epoch 11/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5448 - auc_roc: 0.5864 - acc: 0.7415 - val_loss: 0.5463 - val_auc_roc: 0.5908 - val_acc: 0.7403\n",
      "Epoch 12/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5450 - auc_roc: 0.5948 - acc: 0.7422 - val_loss: 0.5493 - val_auc_roc: 0.5984 - val_acc: 0.7437\n",
      "Epoch 13/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5413 - auc_roc: 0.6019 - acc: 0.7421 - val_loss: 0.5432 - val_auc_roc: 0.6052 - val_acc: 0.7397\n",
      "Epoch 14/50\n",
      "181472/181472 [==============================] - 152s 835us/step - loss: 0.5384 - auc_roc: 0.6084 - acc: 0.7435 - val_loss: 0.5414 - val_auc_roc: 0.6113 - val_acc: 0.7408\n",
      "Epoch 15/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5365 - auc_roc: 0.6141 - acc: 0.7436 - val_loss: 0.5402 - val_auc_roc: 0.6166 - val_acc: 0.7427\n",
      "Epoch 16/50\n",
      "181472/181472 [==============================] - 151s 835us/step - loss: 0.5357 - auc_roc: 0.6191 - acc: 0.7439 - val_loss: 0.5409 - val_auc_roc: 0.6213 - val_acc: 0.7426\n",
      "Epoch 17/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5345 - auc_roc: 0.6235 - acc: 0.7442 - val_loss: 0.5396 - val_auc_roc: 0.6256 - val_acc: 0.7421\n",
      "Epoch 18/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5345 - auc_roc: 0.6275 - acc: 0.7443 - val_loss: 0.5411 - val_auc_roc: 0.6293 - val_acc: 0.7432\n",
      "Epoch 19/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5333 - auc_roc: 0.6311 - acc: 0.7440 - val_loss: 0.5476 - val_auc_roc: 0.6327 - val_acc: 0.7389\n",
      "Epoch 20/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5317 - auc_roc: 0.6342 - acc: 0.7444 - val_loss: 0.5421 - val_auc_roc: 0.6358 - val_acc: 0.7419\n",
      "Epoch 21/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5307 - auc_roc: 0.6373 - acc: 0.7447 - val_loss: 0.5388 - val_auc_roc: 0.6387 - val_acc: 0.7433\n",
      "Epoch 22/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5303 - auc_roc: 0.6401 - acc: 0.7449 - val_loss: 0.5403 - val_auc_roc: 0.6414 - val_acc: 0.7442\n",
      "Epoch 23/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5294 - auc_roc: 0.6427 - acc: 0.7449 - val_loss: 0.5388 - val_auc_roc: 0.6439 - val_acc: 0.7424\n",
      "Epoch 24/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5287 - auc_roc: 0.6451 - acc: 0.7453 - val_loss: 0.5384 - val_auc_roc: 0.6462 - val_acc: 0.7435\n",
      "Epoch 25/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5280 - auc_roc: 0.6473 - acc: 0.7459 - val_loss: 0.5434 - val_auc_roc: 0.6483 - val_acc: 0.7429\n",
      "Epoch 26/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5271 - auc_roc: 0.6494 - acc: 0.7464 - val_loss: 0.5384 - val_auc_roc: 0.6504 - val_acc: 0.7438\n",
      "Epoch 27/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5260 - auc_roc: 0.6514 - acc: 0.7458 - val_loss: 0.5383 - val_auc_roc: 0.6523 - val_acc: 0.7436\n",
      "Epoch 28/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5257 - auc_roc: 0.6533 - acc: 0.7466 - val_loss: 0.5393 - val_auc_roc: 0.6541 - val_acc: 0.7422\n",
      "Epoch 29/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5246 - auc_roc: 0.6550 - acc: 0.7468 - val_loss: 0.5443 - val_auc_roc: 0.6559 - val_acc: 0.7440\n",
      "Epoch 30/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5239 - auc_roc: 0.6567 - acc: 0.7470 - val_loss: 0.5422 - val_auc_roc: 0.6575 - val_acc: 0.7403\n",
      "Epoch 31/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5233 - auc_roc: 0.6583 - acc: 0.7472 - val_loss: 0.5414 - val_auc_roc: 0.6591 - val_acc: 0.7422\n",
      "Epoch 32/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5230 - auc_roc: 0.6598 - acc: 0.7476 - val_loss: 0.5438 - val_auc_roc: 0.6605 - val_acc: 0.7406\n",
      "Epoch 33/50\n",
      "181472/181472 [==============================] - 155s 856us/step - loss: 0.5220 - auc_roc: 0.6613 - acc: 0.7476 - val_loss: 0.5398 - val_auc_roc: 0.6620 - val_acc: 0.7423\n",
      "Epoch 34/50\n",
      "181472/181472 [==============================] - 153s 844us/step - loss: 0.5212 - auc_roc: 0.6627 - acc: 0.7481 - val_loss: 0.5421 - val_auc_roc: 0.6634 - val_acc: 0.7432\n",
      "Epoch 35/50\n",
      "181472/181472 [==============================] - 153s 844us/step - loss: 0.5206 - auc_roc: 0.6641 - acc: 0.7484 - val_loss: 0.5432 - val_auc_roc: 0.6647 - val_acc: 0.7416\n",
      "Epoch 36/50\n",
      "181472/181472 [==============================] - 153s 844us/step - loss: 0.5198 - auc_roc: 0.6654 - acc: 0.7487 - val_loss: 0.5467 - val_auc_roc: 0.6660 - val_acc: 0.7407\n",
      "Epoch 37/50\n",
      "181472/181472 [==============================] - 153s 845us/step - loss: 0.5190 - auc_roc: 0.6666 - acc: 0.7490 - val_loss: 0.5413 - val_auc_roc: 0.6672 - val_acc: 0.7427\n",
      "Epoch 38/50\n",
      "181472/181472 [==============================] - 153s 845us/step - loss: 0.5123 - auc_roc: 0.6679 - acc: 0.7510 - val_loss: 0.5478 - val_auc_roc: 0.6686 - val_acc: 0.7410\n",
      "Epoch 39/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5112 - auc_roc: 0.6693 - acc: 0.7513 - val_loss: 0.5473 - val_auc_roc: 0.6700 - val_acc: 0.7417\n",
      "Epoch 40/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5106 - auc_roc: 0.6707 - acc: 0.7517 - val_loss: 0.5489 - val_auc_roc: 0.6713 - val_acc: 0.7415\n",
      "Epoch 41/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5102 - auc_roc: 0.6720 - acc: 0.7520 - val_loss: 0.5482 - val_auc_roc: 0.6726 - val_acc: 0.7405\n",
      "Epoch 42/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5100 - auc_roc: 0.6732 - acc: 0.7519 - val_loss: 0.5494 - val_auc_roc: 0.6738 - val_acc: 0.7394\n",
      "Epoch 43/50\n",
      "181472/181472 [==============================] - 154s 849us/step - loss: 0.5096 - auc_roc: 0.6744 - acc: 0.7520 - val_loss: 0.5475 - val_auc_roc: 0.6750 - val_acc: 0.7408\n",
      "Epoch 44/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5093 - auc_roc: 0.6756 - acc: 0.7523 - val_loss: 0.5483 - val_auc_roc: 0.6761 - val_acc: 0.7408\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5090 - auc_roc: 0.6766 - acc: 0.7522 - val_loss: 0.5523 - val_auc_roc: 0.6772 - val_acc: 0.7401\n",
      "Epoch 46/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5088 - auc_roc: 0.6777 - acc: 0.7524 - val_loss: 0.5495 - val_auc_roc: 0.6782 - val_acc: 0.7407\n",
      "Epoch 47/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5085 - auc_roc: 0.6787 - acc: 0.7523 - val_loss: 0.5519 - val_auc_roc: 0.6792 - val_acc: 0.7395\n",
      "Epoch 48/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5075 - auc_roc: 0.6797 - acc: 0.7526 - val_loss: 0.5517 - val_auc_roc: 0.6802 - val_acc: 0.7409\n",
      "Epoch 49/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5073 - auc_roc: 0.6806 - acc: 0.7527 - val_loss: 0.5519 - val_auc_roc: 0.6811 - val_acc: 0.7409\n",
      "Epoch 50/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5073 - auc_roc: 0.6815 - acc: 0.7529 - val_loss: 0.5517 - val_auc_roc: 0.6820 - val_acc: 0.7406\n",
      "Fold  9 AUC-LSTM : 0.679253\n",
      "Train Index: [     0      1      2 ... 201631 201632 201633] ,Val Index: [     3      5     32 ... 201604 201623 201629]\n",
      "Creating the multi-model LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:146: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181472 samples, validate on 20162 samples\n",
      "Epoch 1/50\n",
      "181472/181472 [==============================] - 156s 862us/step - loss: 0.6024 - auc_roc: 0.5087 - acc: 0.7094 - val_loss: 0.6005 - val_auc_roc: 0.5208 - val_acc: 0.7095\n",
      "Epoch 2/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.6000 - auc_roc: 0.5294 - acc: 0.7094 - val_loss: 0.5997 - val_auc_roc: 0.5350 - val_acc: 0.7095\n",
      "Epoch 3/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5986 - auc_roc: 0.5398 - acc: 0.7094 - val_loss: 0.5986 - val_auc_roc: 0.5429 - val_acc: 0.7095\n",
      "Epoch 4/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5980 - auc_roc: 0.5455 - acc: 0.7094 - val_loss: 0.5994 - val_auc_roc: 0.5475 - val_acc: 0.7095\n",
      "Epoch 5/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5974 - auc_roc: 0.5494 - acc: 0.7094 - val_loss: 0.5986 - val_auc_roc: 0.5507 - val_acc: 0.7095\n",
      "Epoch 6/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5968 - auc_roc: 0.5522 - acc: 0.7094 - val_loss: 0.5978 - val_auc_roc: 0.5535 - val_acc: 0.7095\n",
      "Epoch 7/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5963 - auc_roc: 0.5547 - acc: 0.7094 - val_loss: 0.5978 - val_auc_roc: 0.5558 - val_acc: 0.7095\n",
      "Epoch 8/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5957 - auc_roc: 0.5569 - acc: 0.7094 - val_loss: 0.5973 - val_auc_roc: 0.5578 - val_acc: 0.7095\n",
      "Epoch 9/50\n",
      "181472/181472 [==============================] - 153s 842us/step - loss: 0.5868 - auc_roc: 0.5595 - acc: 0.7148 - val_loss: 0.5616 - val_auc_roc: 0.5630 - val_acc: 0.7359\n",
      "Epoch 10/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5519 - auc_roc: 0.5684 - acc: 0.7392 - val_loss: 0.5507 - val_auc_roc: 0.5737 - val_acc: 0.7382\n",
      "Epoch 11/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5435 - auc_roc: 0.5789 - acc: 0.7417 - val_loss: 0.5472 - val_auc_roc: 0.5837 - val_acc: 0.7385\n",
      "Epoch 12/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5389 - auc_roc: 0.5885 - acc: 0.7428 - val_loss: 0.5450 - val_auc_roc: 0.5928 - val_acc: 0.7406\n",
      "Epoch 13/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5361 - auc_roc: 0.5970 - acc: 0.7433 - val_loss: 0.5405 - val_auc_roc: 0.6008 - val_acc: 0.7420\n",
      "Epoch 14/50\n",
      "181472/181472 [==============================] - 152s 835us/step - loss: 0.5342 - auc_roc: 0.6044 - acc: 0.7444 - val_loss: 0.5465 - val_auc_roc: 0.6077 - val_acc: 0.7413\n",
      "Epoch 15/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5324 - auc_roc: 0.6109 - acc: 0.7444 - val_loss: 0.5418 - val_auc_roc: 0.6139 - val_acc: 0.7407\n",
      "Epoch 16/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5308 - auc_roc: 0.6167 - acc: 0.7451 - val_loss: 0.5400 - val_auc_roc: 0.6194 - val_acc: 0.7412\n",
      "Epoch 17/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5295 - auc_roc: 0.6220 - acc: 0.7454 - val_loss: 0.5382 - val_auc_roc: 0.6244 - val_acc: 0.7410\n",
      "Epoch 18/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5280 - auc_roc: 0.6267 - acc: 0.7462 - val_loss: 0.5401 - val_auc_roc: 0.6289 - val_acc: 0.7412\n",
      "Epoch 19/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5267 - auc_roc: 0.6310 - acc: 0.7464 - val_loss: 0.5387 - val_auc_roc: 0.6330 - val_acc: 0.7407\n",
      "Epoch 20/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5256 - auc_roc: 0.6350 - acc: 0.7469 - val_loss: 0.5395 - val_auc_roc: 0.6368 - val_acc: 0.7398\n",
      "Epoch 21/50\n",
      "181472/181472 [==============================] - 153s 840us/step - loss: 0.5246 - auc_roc: 0.6385 - acc: 0.7471 - val_loss: 0.5401 - val_auc_roc: 0.6402 - val_acc: 0.7409\n",
      "Epoch 22/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5234 - auc_roc: 0.6419 - acc: 0.7475 - val_loss: 0.5425 - val_auc_roc: 0.6434 - val_acc: 0.7406\n",
      "Epoch 23/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5222 - auc_roc: 0.6449 - acc: 0.7484 - val_loss: 0.5403 - val_auc_roc: 0.6464 - val_acc: 0.7394\n",
      "Epoch 24/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5210 - auc_roc: 0.6479 - acc: 0.7491 - val_loss: 0.5409 - val_auc_roc: 0.6492 - val_acc: 0.7403\n",
      "Epoch 25/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5200 - auc_roc: 0.6506 - acc: 0.7491 - val_loss: 0.5467 - val_auc_roc: 0.6518 - val_acc: 0.7354\n",
      "Epoch 26/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5186 - auc_roc: 0.6531 - acc: 0.7500 - val_loss: 0.5399 - val_auc_roc: 0.6543 - val_acc: 0.7410\n",
      "Epoch 27/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5174 - auc_roc: 0.6555 - acc: 0.7503 - val_loss: 0.5422 - val_auc_roc: 0.6567 - val_acc: 0.7399\n",
      "Epoch 28/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5096 - auc_roc: 0.6580 - acc: 0.7541 - val_loss: 0.5446 - val_auc_roc: 0.6593 - val_acc: 0.7392\n",
      "Epoch 29/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5079 - auc_roc: 0.6606 - acc: 0.7548 - val_loss: 0.5460 - val_auc_roc: 0.6618 - val_acc: 0.7381\n",
      "Epoch 30/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5069 - auc_roc: 0.6630 - acc: 0.7553 - val_loss: 0.5475 - val_auc_roc: 0.6642 - val_acc: 0.7385\n",
      "Epoch 31/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5063 - auc_roc: 0.6653 - acc: 0.7557 - val_loss: 0.5471 - val_auc_roc: 0.6664 - val_acc: 0.7388\n",
      "Epoch 32/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5057 - auc_roc: 0.6675 - acc: 0.7556 - val_loss: 0.5485 - val_auc_roc: 0.6685 - val_acc: 0.7374\n",
      "Epoch 33/50\n",
      "181472/181472 [==============================] - 152s 838us/step - loss: 0.5052 - auc_roc: 0.6695 - acc: 0.7559 - val_loss: 0.5481 - val_auc_roc: 0.6705 - val_acc: 0.7379\n",
      "Epoch 34/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5046 - auc_roc: 0.6715 - acc: 0.7563 - val_loss: 0.5508 - val_auc_roc: 0.6724 - val_acc: 0.7377\n",
      "Epoch 35/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5042 - auc_roc: 0.6733 - acc: 0.7565 - val_loss: 0.5518 - val_auc_roc: 0.6742 - val_acc: 0.7370\n",
      "Epoch 36/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5037 - auc_roc: 0.6750 - acc: 0.7566 - val_loss: 0.5506 - val_auc_roc: 0.6759 - val_acc: 0.7385\n",
      "Epoch 37/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5032 - auc_roc: 0.6767 - acc: 0.7570 - val_loss: 0.5495 - val_auc_roc: 0.6775 - val_acc: 0.7382\n",
      "Epoch 38/50\n",
      "181472/181472 [==============================] - 153s 840us/step - loss: 0.5017 - auc_roc: 0.6782 - acc: 0.7578 - val_loss: 0.5528 - val_auc_roc: 0.6790 - val_acc: 0.7372\n",
      "Epoch 39/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5016 - auc_roc: 0.6798 - acc: 0.7577 - val_loss: 0.5526 - val_auc_roc: 0.6805 - val_acc: 0.7375\n",
      "Epoch 40/50\n",
      "181472/181472 [==============================] - 152s 840us/step - loss: 0.5015 - auc_roc: 0.6812 - acc: 0.7577 - val_loss: 0.5535 - val_auc_roc: 0.6819 - val_acc: 0.7376\n",
      "Epoch 41/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5014 - auc_roc: 0.6826 - acc: 0.7576 - val_loss: 0.5529 - val_auc_roc: 0.6833 - val_acc: 0.7374\n",
      "Epoch 42/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5013 - auc_roc: 0.6839 - acc: 0.7578 - val_loss: 0.5532 - val_auc_roc: 0.6845 - val_acc: 0.7374\n",
      "Epoch 43/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5013 - auc_roc: 0.6852 - acc: 0.7579 - val_loss: 0.5529 - val_auc_roc: 0.6857 - val_acc: 0.7376\n",
      "Epoch 44/50\n",
      "181472/181472 [==============================] - 152s 837us/step - loss: 0.5012 - auc_roc: 0.6863 - acc: 0.7578 - val_loss: 0.5535 - val_auc_roc: 0.6869 - val_acc: 0.7374\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5012 - auc_roc: 0.6875 - acc: 0.7579 - val_loss: 0.5533 - val_auc_roc: 0.6880 - val_acc: 0.7374\n",
      "Epoch 46/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5011 - auc_roc: 0.6885 - acc: 0.7579 - val_loss: 0.5531 - val_auc_roc: 0.6891 - val_acc: 0.7379\n",
      "Epoch 47/50\n",
      "181472/181472 [==============================] - 152s 836us/step - loss: 0.5010 - auc_roc: 0.6896 - acc: 0.7579 - val_loss: 0.5536 - val_auc_roc: 0.6901 - val_acc: 0.7374\n",
      "Epoch 48/50\n",
      "181472/181472 [==============================] - 154s 850us/step - loss: 0.5009 - auc_roc: 0.6905 - acc: 0.7580 - val_loss: 0.5536 - val_auc_roc: 0.6910 - val_acc: 0.7373\n",
      "Epoch 49/50\n",
      "181472/181472 [==============================] - 153s 841us/step - loss: 0.5008 - auc_roc: 0.6915 - acc: 0.7580 - val_loss: 0.5536 - val_auc_roc: 0.6920 - val_acc: 0.7374\n",
      "Epoch 50/50\n",
      "181472/181472 [==============================] - 152s 839us/step - loss: 0.5008 - auc_roc: 0.6924 - acc: 0.7580 - val_loss: 0.5536 - val_auc_roc: 0.6928 - val_acc: 0.7375\n",
      "Fold 10 AUC-LSTM : 0.682130\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits= 10, shuffle=True)\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "sub_preds = np.zeros(rx.shape[0])\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, Y)):\n",
    "\n",
    "    train_x, train_y = X[train_idx,:], Y[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx,:], Y[valid_idx]\n",
    "    train_wx = wX[train_idx,:]\n",
    "    valid_wx = wX[valid_idx,:]\n",
    "    train_id, valid_id = Xid[train_idx], Xid[valid_idx]\n",
    "\n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    if n_fold >= 0:\n",
    "        lstmmodel=train_lstm(n_symbols, embedding_weights,train_x, train_y, valid_x, valid_y)\n",
    "#         feats = Model(inputs=lstmmodel.input, outputs=lstmmodel.get_layer('dense1').output)\n",
    "        lstmmodel.save('LSTM_fold_%d.h5'%(n_fold))\n",
    "        \n",
    "        \n",
    "        tmp_valid = lstmmodel.predict(valid_x)\n",
    "        tmp_valid= np.reshape(tmp_valid, [-1])\n",
    "        oof_preds[valid_idx] = tmp_valid\n",
    "        res1 =  np.reshape(lstmmodel.predict(rx), [-1])\n",
    "        sub_preds += (res1) / folds.n_splits\n",
    "        \n",
    "\n",
    "        print('Fold %2d AUC-LSTM : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        \n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        \n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "preds = pd.DataFrame({\"order_id\":app_test[\"order_id\"], \"deal_or_not\":sub_preds})\n",
    "# create output sub-folder\n",
    "preds.to_csv(\"output/LSTM_\" + str(roc_auc_score(Y, oof_preds)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index: [ 91034 102071   8807 ... 197629  74153 200479] ,Val Index: [177901  30518 191480  67706 164426 153939 127370  86998  85444 172402\n",
      " 190974 187102 149877 141819  85437  46263 127475  42836 179290 105981\n",
      "  64053 119425 191635 145877  44326 164821  44417 128515  11976 136094\n",
      "  33691  78842 197582 181768   2594  49228  74431 182739  34700 156744\n",
      "  50498  77965 160363  20750 133106  32178 158197  82506  18264  41936\n",
      "  17047 179908 127485 197630 135225 188643 200639  89466 109988   9723\n",
      "  64309  28588  30484  92552 164072 170478  29047 193281 147501 200953\n",
      "  46090 139683 137867 192115 169139 188532 171403  32914  19593 116001\n",
      "  65011  18261  30457 169033  75054 102900 128878 142955  95581  37707\n",
      " 141259  30334 169351 155088 190494  91669  35179 137010  30760  76351\n",
      "  98719  25699  25615  64852 122899 165487 185003 142770 110000  66221\n",
      "  43858  13427 172058 152716   5239  21114 116878  79194  29430 103613\n",
      "  53104  84285  39126  20641  18260  16907  91806 141025 120375  42847\n",
      " 115564 166119 196532   3791 125697 191056 164360   4803  69160 157503\n",
      " 163383 193085 160129  97439  12100  68892  51909 175552 125762  19142\n",
      "  50611  42121  45209 198546  50395 124305  88420  47493  89530 140983\n",
      "  91256   6063   8577  56125  41194  97964  51938 198517 101578 191096\n",
      " 199234 106821  81854  45345 143347  25158 124662 131291   4843  84973\n",
      "  55759 177571 134998  17401  48167  32508 130110 179034 153899    398\n",
      "  52181 100470 159365  70148  69549  80039  41628  84287  49448 163947\n",
      " 147257 193136  41032  10659 148461 183997  10988 111213  23552 160245\n",
      "    127 183210  88150 132706  82533 129587  14695  37095  16224 132339\n",
      "  18958  28601 150288  10241  50889 171351  83236 182628  71172 156614\n",
      "   1457 135410 111664 174945 126756 143511 162708  77252 150139 159467\n",
      " 198227 103459  54576  96896 101800 136521  65596 114375  18337  42194\n",
      "  68099  59563  42002   4520  78730  20682  55080 171720 106667  90174\n",
      "  38116  12992  50961  51980  50304  72216  63366 149109  98201 194282\n",
      "  62323 158795  29066  56616  66871  13646 162632  61095  94461 125622\n",
      " 176931 160401  19690  97003  26231  21508  31171 146311 164547  33532\n",
      "  26303  71653  28001 144342  97155  15220  96201  86479 121740  52394\n",
      "  54657  86541 128638 177408  60969 176008 142015 122264 138029 128060\n",
      " 197472  22814 179169  16050 194183   3295  27302  53580  71187 174028\n",
      "  36615  72689 111250  86722  72292  44633 161348  24872 152148  14313\n",
      " 149653 152374  60530    747   4254 110902  96026  87751 150033 180780\n",
      " 173881 109905  87455  48620  28343 181500  27986 118636 189884 115994\n",
      "  96596 108098  50133  34954  64381  74883  22786  89079  31941  98997\n",
      " 186507 136181 134572 163761 110988  97562  96154  43406  74384  75891\n",
      " 156711 127156 162912  79623  28206 152705  83524 100995  86597  23590\n",
      "  53170  35772 143066  98403  99786  41426  77819 192159  25839  74472\n",
      " 144617  54870 150795  66436 188850  67412  94375 106394   6361   4283\n",
      " 187483  58888  23331  79082  88441 121975 176857  98457  13985 172767\n",
      "  35825 158144 163014 145538 130794  48464  98316 167420 136142 175343\n",
      "  58355 196657  52242 105410  80470  71894  80900   9845   8056  58620\n",
      "  87288 112449  37937 129764 200987  43077 101512  17859  18711  71489\n",
      "  71394 113234  63258  12166  39433  29535  35125 187078  60412 133602\n",
      "  62924  35673   3019 126376 154971  52720  93454 130656  96402  29369\n",
      "  11032 163145 196856  22867  36135 183820 174197 149704 171222  53877\n",
      "  12473 152946 153987 190259 137434  36620  82774  31202  47984 113609\n",
      " 133926  87952 122156 170554 143004  97359  12486  12061 157745  70770\n",
      " 114075  13476 176075 130551 116277 110653 192780  99543   8974 120142\n",
      "  90832 157469  99278  20072 130977  82873 118082 132402  29536 159200\n",
      " 192328 164061 180736 163773 158382  27487 157215 190050  58166  41484\n",
      "  58674 104482  72643   8738 100165  76277  40491 197413  98857 108773\n",
      "  39861  99950  28589  15249  20480 198638 110035  68143  86422 128055\n",
      " 111969 107062 186554 131204  28157 149631  56810 141306  78269 135807\n",
      "  96987  95819 183795 140047  10367  98063 155697 142613 132677 187897\n",
      "  93378 186225   2133  11683  56427  63040 100183  12465 192801 149594\n",
      "  70389  54827 137494 197943 141532 171564  95826 112123 199553   2698\n",
      " 139122 103955   7512 157294 107202  40318  19979   4974  84851  16542\n",
      " 142483 141468 167385 197078  47622  86909 106783 125113  31241 179572\n",
      " 186426  29296 163612  36393 133442  64779 178074  14190  77174 176007\n",
      "  13771 155669 149995  75610 176746  83916 199075 116357 104507 161695\n",
      "  78852  22123  64650 135866 195667 103704  93767  62710  27826  56400\n",
      "  90278 105603  79465  77046  76018  15325 127646 191086 124065  96643\n",
      " 173223 157922 131778  53789 189731  41748 105441  91861  75120  39512\n",
      "  74385  53651  16534 195792 122921 181637 167241 185831 112778 130826\n",
      " 145864  20661  32711  83918  85672 142415  59464  40856 181946  23793\n",
      " 178321  31061 185273 109412  57802  75296 128761  38428  70547 164520\n",
      "  90359 135445 173468  87419  16037  74314  55873 158950 198841  84901\n",
      "   2882    696  79518   5827  23390  81059 134875   1762  76917   8324\n",
      "  72497  64156 121525  40907  52323 181810 150244 176756 189160 176092\n",
      " 170306 163314 146059  23668 116589  35064 179446  84786   2316  80346\n",
      "  15580  96946 146593 197831   9152 129351  78796 123020  97871  56064\n",
      " 197221 121895  19143 143550 119153  32824  86336 181303 150366 137755\n",
      "  62938  44866  98340 163013 170071  29297  87624 101686 166546  20523\n",
      " 171128 147989 180284 100707 172980  44261 124486   3842  68534  93316\n",
      "  23224  43122  14178 171547 124433   4825 144838 102084 148116 103222\n",
      "  57869 130469  55919 169817  44601 171732  66417 179611  92886 174042\n",
      "  60549  93197   8138 190883  65037 102277 149345 188697   1404 188280\n",
      "  70470 108119 102876 172408 139531  71216  20301 151922 125455 157785\n",
      "  84607  70402 134608 112276 123647  11685  89814  88166  10707  53274\n",
      " 168416 166538  64260 151258  83402 151150 174346  80402 137204 114118\n",
      " 130307 183343 128771 129785  71727 157636 124991 187447 113641 127020\n",
      " 101817 151851  87519 149851  77660 130611  77844 124061  32190  12668\n",
      " 179017 145139 199875 158703 162250  55722 134935  73299 147825  19281\n",
      " 179208 114405 178114 111245  99400  78757  48122  79051  74893 167985\n",
      " 178166   7974 166528 155164 198126 118939  88373  99613 129223  82195\n",
      " 186498 122861  24248 145398 149597  77535  76628 125928 122122 113347\n",
      " 147246 148011  43708  29422 183877  64335   9121  15290  78700  87863\n",
      "   5693 185164 177982 132845 164658 164051 150789 137824 107528  93917\n",
      " 189084 139118  15227 143764  16522 102640  71285  56642  49572 115584\n",
      "  19131 159569  51771 182454 171109 103484 136278 131380 126055 123809\n",
      "  21436 176065 148200  42414  18512  86546   4968 129725 124482  61977\n",
      " 164071  96712 109710  18386  25110  91618 115875  19879 128842  42106\n",
      " 113835 115729  15953  22480  66877  37352  18686 186435 186635 114039\n",
      "  61867 108067 149068 143739 188270 176687  65789  49119   3743  97736\n",
      "  94554 116116 115743 129233 120728 106934  17840 118734 191642  29322\n",
      "  35063 172396 109197  20822  18138  75030 191332  43686 167860  18884\n",
      " 176420 171176 165555 193246  39668 164655 120910  64709  26150 187074\n",
      "  44788  33687 186413  36864 105862 173139 103442  20095 169847  47692]\n",
      "Creating the multi-model LSTM model...\n",
      "Start to train a model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:150: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200634 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "200634/200634 [==============================] - 163s 811us/step - loss: 0.6021 - auc_roc: 0.5091 - acc: 0.7097 - val_loss: 0.6474 - val_auc_roc: 0.5239 - val_acc: 0.6570\n",
      "Epoch 2/30\n",
      "200634/200634 [==============================] - 157s 783us/step - loss: 0.5994 - auc_roc: 0.5313 - acc: 0.7097 - val_loss: 0.6469 - val_auc_roc: 0.5367 - val_acc: 0.6570\n",
      "Epoch 3/30\n",
      "200634/200634 [==============================] - 158s 789us/step - loss: 0.5987 - auc_roc: 0.5403 - acc: 0.7097 - val_loss: 0.6450 - val_auc_roc: 0.5426 - val_acc: 0.6570\n",
      "Epoch 4/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5978 - auc_roc: 0.5449 - acc: 0.7097 - val_loss: 0.6447 - val_auc_roc: 0.5468 - val_acc: 0.6570\n",
      "Epoch 5/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5974 - auc_roc: 0.5484 - acc: 0.7097 - val_loss: 0.6451 - val_auc_roc: 0.5497 - val_acc: 0.6570\n",
      "Epoch 6/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5969 - auc_roc: 0.5510 - acc: 0.7097 - val_loss: 0.6427 - val_auc_roc: 0.5522 - val_acc: 0.6570\n",
      "Epoch 7/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5963 - auc_roc: 0.5534 - acc: 0.7097 - val_loss: 0.6407 - val_auc_roc: 0.5542 - val_acc: 0.6570\n",
      "Epoch 8/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5778 - auc_roc: 0.5574 - acc: 0.7211 - val_loss: 0.5949 - val_auc_roc: 0.5625 - val_acc: 0.7000\n",
      "Epoch 9/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5483 - auc_roc: 0.5689 - acc: 0.7400 - val_loss: 0.5874 - val_auc_roc: 0.5749 - val_acc: 0.6960\n",
      "Epoch 10/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5425 - auc_roc: 0.5805 - acc: 0.7415 - val_loss: 0.5947 - val_auc_roc: 0.5859 - val_acc: 0.6920\n",
      "Epoch 11/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5389 - auc_roc: 0.5908 - acc: 0.7424 - val_loss: 0.5825 - val_auc_roc: 0.5953 - val_acc: 0.6950\n",
      "Epoch 12/30\n",
      "200634/200634 [==============================] - 157s 783us/step - loss: 0.5368 - auc_roc: 0.5994 - acc: 0.7434 - val_loss: 0.5936 - val_auc_roc: 0.6034 - val_acc: 0.6910\n",
      "Epoch 13/30\n",
      "200634/200634 [==============================] - 158s 785us/step - loss: 0.5348 - auc_roc: 0.6070 - acc: 0.7435 - val_loss: 0.5848 - val_auc_roc: 0.6104 - val_acc: 0.6910\n",
      "Epoch 14/30\n",
      "200634/200634 [==============================] - 157s 784us/step - loss: 0.5331 - auc_roc: 0.6135 - acc: 0.7438 - val_loss: 0.5791 - val_auc_roc: 0.6165 - val_acc: 0.6920\n",
      "Epoch 15/30\n",
      "200634/200634 [==============================] - 158s 786us/step - loss: 0.5321 - auc_roc: 0.6193 - acc: 0.7447 - val_loss: 0.5812 - val_auc_roc: 0.6219 - val_acc: 0.6880\n",
      "Epoch 16/30\n",
      "200634/200634 [==============================] - 158s 785us/step - loss: 0.5306 - auc_roc: 0.6244 - acc: 0.7449 - val_loss: 0.5825 - val_auc_roc: 0.6267 - val_acc: 0.6880\n",
      "Epoch 17/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5296 - auc_roc: 0.6290 - acc: 0.7453 - val_loss: 0.5837 - val_auc_roc: 0.6310 - val_acc: 0.6920\n",
      "Epoch 18/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5287 - auc_roc: 0.6330 - acc: 0.7453 - val_loss: 0.5793 - val_auc_roc: 0.6348 - val_acc: 0.6880\n",
      "Epoch 19/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5280 - auc_roc: 0.6366 - acc: 0.7464 - val_loss: 0.5787 - val_auc_roc: 0.6383 - val_acc: 0.6900\n",
      "Epoch 20/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5266 - auc_roc: 0.6400 - acc: 0.7467 - val_loss: 0.5822 - val_auc_roc: 0.6416 - val_acc: 0.6880\n",
      "Epoch 21/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5259 - auc_roc: 0.6431 - acc: 0.7468 - val_loss: 0.5860 - val_auc_roc: 0.6445 - val_acc: 0.6890\n",
      "Epoch 22/30\n",
      "200634/200634 [==============================] - 157s 783us/step - loss: 0.5250 - auc_roc: 0.6460 - acc: 0.7469 - val_loss: 0.5837 - val_auc_roc: 0.6473 - val_acc: 0.6890\n",
      "Epoch 23/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5242 - auc_roc: 0.6486 - acc: 0.7470 - val_loss: 0.5887 - val_auc_roc: 0.6499 - val_acc: 0.6890\n",
      "Epoch 24/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5233 - auc_roc: 0.6511 - acc: 0.7474 - val_loss: 0.5868 - val_auc_roc: 0.6523 - val_acc: 0.6910\n",
      "Epoch 25/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5221 - auc_roc: 0.6534 - acc: 0.7475 - val_loss: 0.5840 - val_auc_roc: 0.6546 - val_acc: 0.6920\n",
      "Epoch 26/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5219 - auc_roc: 0.6557 - acc: 0.7480 - val_loss: 0.5961 - val_auc_roc: 0.6567 - val_acc: 0.6920\n",
      "Epoch 27/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5207 - auc_roc: 0.6577 - acc: 0.7485 - val_loss: 0.5765 - val_auc_roc: 0.6587 - val_acc: 0.6920\n",
      "Epoch 28/30\n",
      "200634/200634 [==============================] - 157s 781us/step - loss: 0.5197 - auc_roc: 0.6596 - acc: 0.7487 - val_loss: 0.5862 - val_auc_roc: 0.6606 - val_acc: 0.6930\n",
      "Epoch 29/30\n",
      "200634/200634 [==============================] - 157s 782us/step - loss: 0.5195 - auc_roc: 0.6615 - acc: 0.7489 - val_loss: 0.5891 - val_auc_roc: 0.6623 - val_acc: 0.6870\n",
      "Epoch 30/30\n",
      "200634/200634 [==============================] - 157s 783us/step - loss: 0.5183 - auc_roc: 0.6632 - acc: 0.7491 - val_loss: 0.5752 - val_auc_roc: 0.6641 - val_acc: 0.6920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "len1 = len(Y)\n",
    "tind = np.zeros(len1, np.int)\n",
    "for i in range(len1):\n",
    "    tind[i]=i\n",
    "import random as rn\n",
    "rn.shuffle(tind)\n",
    "\n",
    "train_x, train_y = X[tind[1000:],:], Y[tind[1000:]]\n",
    "valid_x, valid_y = X[tind[:1000],:], Y[tind[:1000]]\n",
    "\n",
    "print(\"Train Index:\",tind[1000:],\",Val Index:\",tind[:1000])\n",
    "\n",
    "lstmmodel=train_lstm(n_symbols, embedding_weights,train_x, train_y, valid_x, valid_y)\n",
    "lstmmodel.save('LSTM_%d.h5'%(n_fold))\n",
    "\n",
    "\n",
    "tmp_valid = lstmmodel.predict(valid_x)\n",
    "tmp_valid= np.reshape(tmp_valid, [-1])\n",
    "res1 =  np.reshape(lstmmodel.predict(rx), [-1])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "preds = pd.DataFrame({\"order_id\":app_test[\"order_id\"], \"deal_or_not\":res1})\n",
    "# create output sub-folder\n",
    "preds.to_csv(\"output/LSTM_all.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "preds = pd.DataFrame({\"order_id\":app_test[\"order_id\"], \"deal_or_not\":res1})\n",
    "# create output sub-folder\n",
    "preds.to_csv(\"output/LSTM_all.csv\", index=False)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
