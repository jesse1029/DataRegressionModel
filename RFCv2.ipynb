{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import jieba, pdb\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "def word2vec(x):\n",
    "    words = jieba.cut(str(x), cut_all=False)\n",
    "    vec = np.zeros((4))\n",
    "    cnt = 1\n",
    "\n",
    "    for word in words:\n",
    "        if (word not in stopword_set) and (word != ' ') and (word in model.wv.vocab):\n",
    "            vec += model[word]\n",
    "            cnt +=1\n",
    "    vec /= cnt\n",
    "    return vec\n",
    "\n",
    "def cv(x):\n",
    "    x=x.replace('\\n','')\n",
    "    xx=x.split(' ')\n",
    "    y=np.zeros((64))\n",
    "    for i in range(64):          \n",
    "        print(xx[i+1])\n",
    "        y[i]=float(xx[i+1])          \n",
    "    return y\n",
    "\n",
    "def Convert_Date(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = pd.to_datetime(Year+'-'+Month+'-'+Day)\n",
    "    return date1\n",
    "\n",
    "def Convert_orderid(x):\n",
    "    return str(x).strip('\\n')\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_order = pd.read_csv(\"dataset/order.csv\")\n",
    "df_group = pd.read_csv(\"dataset/group.csv\")\n",
    "df_airline = pd.read_csv(\"dataset/airline.csv\")\n",
    "df_day_schedule = pd.read_csv(\"day_schedule_processed.txt\")\n",
    "df_train = pd.read_csv(\"training-set.csv\")\n",
    "df_test = pd.read_csv(\"val.csv\")\n",
    "df_result = pd.read_csv(\"testing-set.csv\")\n",
    "# date Conversion\n",
    "\n",
    "month = {'Jan': '01', 'Feb': '02' , 'Mar':'03' ,'Apr': '04', \n",
    "'May': '05', 'Jun': '06' , 'Jul': '07' , 'Aug':'08', \n",
    "'Sep':'09', 'Oct':'10' , 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "# group data\n",
    "df_group['Begin_Date']=df_group.begin_date.apply(lambda x: Convert_Date(x))\n",
    "df_group['Begin_Tick']=df_group.begin_date.apply(lambda x: Date2Ticks(x))\n",
    "df_group['SubLine']= df_group.sub_line.apply(lambda x: int(x[14:]))\n",
    "df_group['Area']= df_group.area.apply(lambda x: int(x[11:]))\n",
    "df_group['name']= df_group.area.apply(lambda x: len(x))\n",
    "df_group['group_id']=df_group.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_airline['group_id']=df_airline.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order['group_id']=df_order.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_day_schedule['group_id']=df_day_schedule.group_id.apply(lambda x: Convert_orderid(x))\n",
    "\n",
    "\n",
    "group_used_cols=['group_id','Begin_Date','Begin_Tick','days','Area','SubLine','price', 'name']\n",
    "df_train['order_id']=df_train.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_result['order_id']=df_result.order_id.apply(lambda x: Convert_orderid(x))\n",
    "\n",
    "df_order_1 = df_order.merge(df_group[group_used_cols], on='group_id')\n",
    "# for order data\n",
    "df_order_1['Order_Date']=df_order_1.order_date.apply(lambda x: Convert_Date(x))\n",
    "df_order_1['Order_Tick']=df_order_1.order_date.apply(lambda x: Date2Ticks(x))\n",
    "df_order_1['order_id']=df_order_1.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order_1['Source_1']= df_order_1.source_1.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Source_2']= df_order_1.source_2.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Unit']= df_order_1.unit.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Begin_Date']=pd.to_datetime(df_order_1['Begin_Date'])\n",
    "df_order_1['Order_Date']=pd.to_datetime(df_order_1['Order_Date'])\n",
    "df_order_1['PreDays']=(df_order_1['Begin_Date']-df_order_1['Order_Date']).dt.days\n",
    "df_order_1['Begin_Date_Weekday']= df_order_1['Begin_Date'].dt.dayofweek\n",
    "df_order_1['Order_Date_Weekday']= df_order_1['Order_Date'].dt.dayofweek\n",
    "df_order_1['Return_Date_Weekday']= (df_order_1['Begin_Date'].dt.dayofweek+df_order_1['days'])%7\n",
    "df_order_1['tick_diff'] = (df_order_1['Begin_Tick'] - df_order_1['Order_Tick'])/10000\n",
    "df_order_1['price'] = df_order_1['price']/1000\n",
    "\n",
    "order_used_columns=['order_id', 'group_id','tick_diff', 'Source_1', 'Source_2', 'Unit',\n",
    "'people_amount', 'days', 'Area', 'SubLine', 'price',\n",
    "'PreDays','Begin_Date_Weekday', 'Order_Date_Weekday', 'Return_Date_Weekday', 'name']\n",
    "\n",
    "df_order_2=df_order_1[order_used_columns].merge(df_day_schedule[['group_id','title']], on='group_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201634, 14)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n",
    "\n",
    "n_symbols = len(index_dict) + 1  # 索引数字的个数，因为有的词语索引为0，所以+1\n",
    "embedding_weights = np.zeros((n_symbols, 100))  # 创建l一个n_symbols * 100的0矩阵\n",
    "for w, index in index_dict.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "    embedding_weights[index, :] = word_vectors[w]  # 词向量矩阵，第一行是0向量（没有索引为0的词语，未被填充）\n",
    "\n",
    "# train/test data\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "\n",
    "Y=df_train_1['deal_or_not'].values.tolist()\n",
    "swX_tmp = df_train_1['title'].values.tolist()\n",
    "Xid = df_train_1['order_id'].values.tolist()\n",
    "del df_train_1['deal_or_not'] \n",
    "del df_train_1['title']\n",
    "del df_train_1['group_id'] \n",
    "del df_train_1['order_id']\n",
    "X = df_train_1.values.tolist()\n",
    "\n",
    "rid = df_result_1['order_id'].values.tolist()\n",
    "swrx = df_result_1['title'].values.tolist()\n",
    "del df_result_1['deal_or_not']\n",
    "del df_result_1['title']\n",
    "del df_result_1['order_id']\n",
    "del df_result_1['group_id']\n",
    "\n",
    "rx = df_result_1.values.tolist()\n",
    "\n",
    "\n",
    "sX, sY, Xid =np.asarray(X), np.asarray(Y), np.asarray(Xid)\n",
    "rx,rid = np.asarray(rx), np.asarray(rid)\n",
    "X,Y, swX=[],[], []\n",
    "for i in range(len(sY)):\n",
    "    if (int(Xid[i])<=204000):\n",
    "        X.append(sX[i,:])\n",
    "        Y.append(sY[i])\n",
    "        swX.append(swX_tmp[i])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in str(sen):\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "wX = text_to_index_array(index_dict, swX)\n",
    "wrx = text_to_index_array(index_dict, swrx)\n",
    "wX = sequence.pad_sequences(wX, maxlen=140)\n",
    "wrx = sequence.pad_sequences(wrx, maxlen=140)\n",
    "\n",
    "\n",
    "# X=np.concatenate([X, wX], axis=1)\n",
    "# rx=np.concatenate([rx, wrx], axis=1)\n",
    "\n",
    "    \n",
    "print(X.shape)\n",
    "\n",
    "# np.save(\"data.npy\", [X,Y,rx])\n",
    "# [X,Y,rx] = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [     3      9     14 ..., 201616 201621 201628]\n",
      "Fold  1 AUC : 0.655304\n",
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [    29     30     34 ..., 201615 201617 201619]\n",
      "Fold  2 AUC : 0.653729\n",
      "Train Index: [     2      3      5 ..., 201630 201631 201632] ,Val Index: [     0      1      4 ..., 201622 201629 201633]\n",
      "Fold  3 AUC : 0.656581\n",
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [    10     18     25 ..., 201600 201626 201627]\n",
      "Fold  4 AUC : 0.651918\n",
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [    22     32     33 ..., 201598 201602 201618]\n",
      "Fold  5 AUC : 0.657529\n",
      "Train Index: [     0      1      3 ..., 201631 201632 201633] ,Val Index: [     2      5      7 ..., 201611 201620 201630]\n",
      "Fold  6 AUC : 0.660874\n",
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [    20     24     39 ..., 201588 201606 201624]\n",
      "Fold  7 AUC : 0.662169\n",
      "Train Index: [     0      1      2 ..., 201629 201630 201633] ,Val Index: [    17     21     37 ..., 201623 201631 201632]\n",
      "Fold  8 AUC : 0.656976\n",
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [    41     60     73 ..., 201604 201614 201625]\n",
      "Fold  9 AUC : 0.653790\n",
      "Train Index: [     0      1      2 ..., 201631 201632 201633] ,Val Index: [    42     51     58 ..., 201592 201599 201608]\n",
      "Fold 10 AUC : 0.658093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = StratifiedKFold(n_splits= 10, shuffle=True)\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "sub_preds = np.zeros(rx.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, Y)):\n",
    "\n",
    "    train_x, train_y,train_id = X[train_idx,:], Y[train_idx], Xid[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx,:], Y[valid_idx]\n",
    "    valid_id=Xid[valid_idx]\n",
    "\n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    if n_fold >= 0:\n",
    "        clf = RandomForestRegressor(n_estimators=100, max_features=\"log2\", n_jobs=4)\n",
    "        clf.fit(train_x, train_y)\n",
    "        py = clf.predict(train_x)\n",
    "#         clf2 = RandomForestRegressor(n_estimators=500, max_features=\"log2\",n_jobs=4)\n",
    "#         clf2.fit(train_x, train_y-py)\n",
    "        \n",
    "        tmp_valid = (clf.predict(valid_x))\n",
    "        tmp_valid[tmp_valid>1]=1\n",
    "        tmp_valid[tmp_valid<0]=0\n",
    "        oof_preds[valid_idx] =tmp_valid\n",
    "#         tmp.dump('kfold_' + str(n_fold) + '.pkl')\n",
    "        sub_preds += (clf.predict(rx)) / folds.n_splits\n",
    "        sub_preds[sub_preds>1]=1          \n",
    "        sub_preds[sub_preds<0]=0\n",
    "\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        \n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "preds = pd.DataFrame({\"order_id\":app_test[\"order_id\"], \"deal_or_not\":sub_preds})\n",
    "# create output sub-folder\n",
    "preds.to_csv(\"output/RF_\" + str(roc_auc_score(Y, oof_preds)) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
