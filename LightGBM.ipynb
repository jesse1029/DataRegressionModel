{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import time, datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import gc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import jieba, pdb\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "# load stopwords set\n",
    "stopword_set = set()\n",
    "with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:\n",
    "    for stopword in stopwords:\n",
    "        stopword_set.add(stopword.strip('\\n'))\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"word2vec2.model\")\n",
    "\n",
    "\n",
    "def create_dictionaries(p_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True)\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec\n",
    "\n",
    "def Convert_orderid(x):\n",
    "    return str(x).strip('\\n')\n",
    "\n",
    "def Convert_Date(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = pd.to_datetime(Year+'-'+Month+'-'+Day)\n",
    "    return date1\n",
    "\n",
    "def Date2Ticks(x):\n",
    "    Year='20'+x[-2:]\n",
    "    Month=month[x[-6:-3]]\n",
    "    Day=x[:-7]\n",
    "    date1 = str(Year+'/'+Month+'/'+Day)\n",
    "    return time.mktime(datetime.datetime.strptime(date1, \"%Y/%m/%d\").timetuple())\n",
    "\n",
    "index_dict, word_vectors= create_dictionaries(model)\n",
    "output = open(\"wordwmbedding.pkl\", 'wb')\n",
    "pickle.dump(index_dict, output)  # 索引字典\n",
    "pickle.dump(word_vectors, output)  # 词向量字典\n",
    "output.close()\n",
    "\n",
    "\n",
    "n_symbols = len(index_dict) + 1  # 索引数字的个数，因为有的词语索引为0，所以+1\n",
    "embedding_weights = np.zeros((n_symbols, 100))  # 创建l一个n_symbols * 100的0矩阵\n",
    "for w, index in index_dict.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "    embedding_weights[index, :] = word_vectors[w]  # 词向量矩阵，第一行是0向量（没有索引为0的词语，未被填充）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jess\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "df_order = pd.read_csv(\"dataset/order.csv\")\n",
    "df_group = pd.read_csv(\"dataset/group.csv\")\n",
    "df_airline = pd.read_csv(\"dataset/airline.csv\")\n",
    "df_day_schedule = pd.read_csv(\"day_schedule_processed.txt\")\n",
    "df_train = pd.read_csv(\"training-set.csv\")\n",
    "df_test = pd.read_csv(\"val.csv\")\n",
    "df_result = pd.read_csv(\"testing-set.csv\")\n",
    "# date Conversion\n",
    "\n",
    "month = {'Jan': '01', 'Feb': '02' , 'Mar':'03' ,'Apr': '04', \n",
    "'May': '05', 'Jun': '06' , 'Jul': '07' , 'Aug':'08', \n",
    "'Sep':'09', 'Oct':'10' , 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "# group data\n",
    "df_group['Begin_Date']=df_group.begin_date.apply(lambda x: Convert_Date(x))\n",
    "df_group['Begin_Tick']=df_group.begin_date.apply(lambda x: Date2Ticks(x))\n",
    "df_group['SubLine']= df_group.sub_line.apply(lambda x: int(x[14:]))\n",
    "df_group['Area']= df_group.area.apply(lambda x: int(x[11:]))\n",
    "df_group['name']= df_group.area.apply(lambda x: len(x))\n",
    "df_group['group_id']=df_group.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_airline['group_id']=df_airline.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order['group_id']=df_order.group_id.apply(lambda x: Convert_orderid(x))\n",
    "df_day_schedule['group_id']=df_day_schedule.group_id.apply(lambda x: Convert_orderid(x))\n",
    "\n",
    "\n",
    "group_used_cols=['group_id','Begin_Date','Begin_Tick','days','Area','SubLine','price', 'name']\n",
    "df_train['order_id']=df_train.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_result['order_id']=df_result.order_id.apply(lambda x: Convert_orderid(x))\n",
    "\n",
    "df_order_1 = df_order.merge(df_group[group_used_cols], on='group_id')\n",
    "# for order data\n",
    "df_order_1['Order_Date']=df_order_1.order_date.apply(lambda x: Convert_Date(x))\n",
    "df_order_1['Order_Tick']=df_order_1.order_date.apply(lambda x: Date2Ticks(x))\n",
    "df_order_1['order_id']=df_order_1.order_id.apply(lambda x: Convert_orderid(x))\n",
    "df_order_1['Source_1']= df_order_1.source_1.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Source_2']= df_order_1.source_2.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Unit']= df_order_1.unit.apply(lambda x: int(x[11:]))\n",
    "df_order_1['Begin_Date']=pd.to_datetime(df_order_1['Begin_Date'])\n",
    "df_order_1['Order_Date']=pd.to_datetime(df_order_1['Order_Date'])\n",
    "df_order_1['PreDays']=(df_order_1['Begin_Date']-df_order_1['Order_Date']).dt.days\n",
    "df_order_1['Begin_Date_Weekday']= df_order_1['Begin_Date'].dt.dayofweek\n",
    "df_order_1['Order_Date_Weekday']= df_order_1['Order_Date'].dt.dayofweek\n",
    "df_order_1['Return_Date_Weekday']= (df_order_1['Begin_Date'].dt.dayofweek+df_order_1['days'])%7\n",
    "df_order_1['tick_diff'] = (df_order_1['Begin_Tick'] - df_order_1['Order_Tick'])/10000\n",
    "df_order_1['price'] = df_order_1['price']/1000\n",
    "\n",
    "order_used_columns=['order_id', 'group_id','tick_diff', 'Source_1', 'Source_2', 'Unit',\n",
    "'people_amount', 'days', 'Area', 'SubLine', 'price',\n",
    "'PreDays','Begin_Date_Weekday', 'Order_Date_Weekday', 'Return_Date_Weekday', 'name']\n",
    "\n",
    "df_order_2=df_order_1[order_used_columns].merge(df_day_schedule[['group_id','title']], on='group_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 297020, 99895 training, testing data\n",
      "Got 297019, 99895 training, testing data\n"
     ]
    }
   ],
   "source": [
    "# train/test data\n",
    "print(\"Got %d, %d training, testing data\" % (len(df_train), len(df_result)))\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "print(\"Got %d, %d training, testing data\" % (len(df_train_1), len(df_result_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297019, 14)\n"
     ]
    }
   ],
   "source": [
    "# train/test data\n",
    "df_train_1=df_train.merge(df_order_2,on='order_id')\n",
    "df_result_1=df_result.merge(df_order_2,on='order_id')\n",
    "\n",
    "Y=df_train_1['deal_or_not'].values.tolist()\n",
    "swX_tmp = df_train_1['title'].values.tolist()\n",
    "Xid = df_train_1['order_id'].values.tolist()\n",
    "del df_train_1['deal_or_not'] \n",
    "del df_train_1['title']\n",
    "del df_train_1['group_id'] \n",
    "del df_train_1['order_id']\n",
    "X = df_train_1.values.tolist()\n",
    "\n",
    "rid = df_result_1['order_id'].values.tolist()\n",
    "swrx = df_result_1['title'].values.tolist()\n",
    "del df_result_1['deal_or_not']\n",
    "del df_result_1['title']\n",
    "del df_result_1['order_id']\n",
    "del df_result_1['group_id']\n",
    "\n",
    "rx = df_result_1.values.tolist()\n",
    "\n",
    "\n",
    "sX, sY, Xid =np.asarray(X), np.asarray(Y), np.asarray(Xid)\n",
    "rx,rid = np.asarray(rx), np.asarray(rid)\n",
    "X,Y, swX=[],[], []\n",
    "for i in range(len(sY)):\n",
    "   # if (int(Xid[i])<=204000):\n",
    "        X.append(sX[i,:])\n",
    "        Y.append(sY[i])\n",
    "        swX.append(swX_tmp[i])\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def text_to_index_array(p_new_dic, p_sen):  # 文本转为索引数字模式\n",
    "    new_sentences = []\n",
    "    for sen in p_sen:\n",
    "        new_sen = []\n",
    "        for word in str(sen):\n",
    "            try:\n",
    "                new_sen.append(p_new_dic[word])  # 单词转索引数字\n",
    "            except:\n",
    "                new_sen.append(0)  # 索引字典里没有的词转为数字0\n",
    "        new_sentences.append(new_sen)\n",
    "\n",
    "    return np.array(new_sentences)\n",
    "\n",
    "\n",
    "wX = text_to_index_array(index_dict, swX)\n",
    "wrx = text_to_index_array(index_dict, swrx)\n",
    "wX = sequence.pad_sequences(wX, maxlen=200)\n",
    "wrx = sequence.pad_sequences(wrx, maxlen=200)\n",
    "\n",
    "\n",
    "# X=np.concatenate([X, wX], axis=1)\n",
    "# rx=np.concatenate([rx, wrx], axis=1)\n",
    "\n",
    "    \n",
    "print(X.shape)\n",
    "\n",
    "# np.save(\"data.npy\", [X,Y,rx])\n",
    "# [X,Y,rx] = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index: [     0      1      2 ... 297016 297017 297018] ,Val Index: [     9     10     11 ... 296966 296967 297006]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.691415\n",
      "[10000]\tvalid_0's auc: 0.693339\n",
      "[15000]\tvalid_0's auc: 0.693455\n",
      "[20000]\tvalid_0's auc: 0.6935\n",
      "[25000]\tvalid_0's auc: 0.69351\n",
      "[30000]\tvalid_0's auc: 0.693515\n",
      "Early stopping, best iteration is:\n",
      "[28913]\tvalid_0's auc: 0.693517\n",
      "Fold  1 AUC : 0.693489\n",
      "Train Index: [     0      1      2 ... 297016 297017 297018] ,Val Index: [    12     13     14 ... 296992 297001 297013]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.682632\n",
      "[10000]\tvalid_0's auc: 0.683283\n",
      "Early stopping, best iteration is:\n",
      "[9042]\tvalid_0's auc: 0.68334\n",
      "Fold  2 AUC : 0.683363\n",
      "Train Index: [     0      1      2 ... 297014 297015 297016] ,Val Index: [     8     16     18 ... 297012 297017 297018]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.685519\n",
      "[10000]\tvalid_0's auc: 0.686439\n",
      "[15000]\tvalid_0's auc: 0.68649\n",
      "Early stopping, best iteration is:\n",
      "[10808]\tvalid_0's auc: 0.68651\n",
      "Fold  3 AUC : 0.686449\n",
      "Train Index: [     1      3      5 ... 297016 297017 297018] ,Val Index: [     0      2      4 ... 296991 296995 297010]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.686459\n",
      "[10000]\tvalid_0's auc: 0.688101\n",
      "[15000]\tvalid_0's auc: 0.688201\n",
      "Early stopping, best iteration is:\n",
      "[13518]\tvalid_0's auc: 0.688267\n",
      "Fold  4 AUC : 0.688299\n",
      "Train Index: [     0      1      2 ... 297016 297017 297018] ,Val Index: [     5     19     29 ... 296982 296985 296996]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.681117\n",
      "[10000]\tvalid_0's auc: 0.681811\n",
      "[15000]\tvalid_0's auc: 0.682024\n",
      "[20000]\tvalid_0's auc: 0.682072\n",
      "[25000]\tvalid_0's auc: 0.682058\n",
      "Early stopping, best iteration is:\n",
      "[22572]\tvalid_0's auc: 0.682077\n",
      "Fold  5 AUC : 0.681969\n",
      "Train Index: [     0      1      2 ... 297014 297017 297018] ,Val Index: [     3     22     39 ... 297011 297015 297016]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.683583\n",
      "[10000]\tvalid_0's auc: 0.685191\n",
      "[15000]\tvalid_0's auc: 0.685362\n",
      "Early stopping, best iteration is:\n",
      "[14427]\tvalid_0's auc: 0.685419\n",
      "Fold  6 AUC : 0.685313\n",
      "Train Index: [     0      2      3 ... 297016 297017 297018] ,Val Index: [     1     21     25 ... 296989 296998 297014]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.691674\n",
      "[10000]\tvalid_0's auc: 0.694027\n",
      "[15000]\tvalid_0's auc: 0.694102\n",
      "[20000]\tvalid_0's auc: 0.694107\n",
      "Early stopping, best iteration is:\n",
      "[16858]\tvalid_0's auc: 0.694155\n",
      "Fold  7 AUC : 0.694285\n",
      "Train Index: [     0      1      2 ... 297016 297017 297018] ,Val Index: [     6     28     47 ... 296979 296993 296997]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.684656\n",
      "[10000]\tvalid_0's auc: 0.686153\n",
      "[15000]\tvalid_0's auc: 0.686196\n",
      "Early stopping, best iteration is:\n",
      "[11782]\tvalid_0's auc: 0.686274\n",
      "Fold  8 AUC : 0.686178\n",
      "Train Index: [     0      1      2 ... 297016 297017 297018] ,Val Index: [    32     37     50 ... 297004 297007 297008]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.685708\n",
      "[10000]\tvalid_0's auc: 0.6876\n",
      "[15000]\tvalid_0's auc: 0.687632\n",
      "Early stopping, best iteration is:\n",
      "[13528]\tvalid_0's auc: 0.687667\n",
      "Fold  9 AUC : 0.687631\n",
      "Train Index: [     0      1      2 ... 297016 297017 297018] ,Val Index: [    23     30     41 ... 296986 296987 297002]\n",
      "Training until validation scores don't improve for 5000 rounds.\n",
      "[5000]\tvalid_0's auc: 0.6811\n",
      "[10000]\tvalid_0's auc: 0.682823\n",
      "[15000]\tvalid_0's auc: 0.683181\n",
      "[20000]\tvalid_0's auc: 0.683347\n",
      "[25000]\tvalid_0's auc: 0.68338\n",
      "[30000]\tvalid_0's auc: 0.683401\n",
      "[35000]\tvalid_0's auc: 0.683425\n",
      "[40000]\tvalid_0's auc: 0.683435\n",
      "[45000]\tvalid_0's auc: 0.683433\n",
      "Early stopping, best iteration is:\n",
      "[40342]\tvalid_0's auc: 0.683438\n",
      "Fold 10 AUC : 0.683359\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folds = StratifiedKFold(n_splits= 10, shuffle=True)\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "sub_preds = np.zeros(rx.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X, Y)):\n",
    "\n",
    "    train_x, train_y,train_id = X[train_idx,:], Y[train_idx], Xid[train_idx]\n",
    "    valid_x, valid_y = X[valid_idx,:], Y[valid_idx]\n",
    "    valid_id=Xid[valid_idx]\n",
    "\n",
    "    print(\"Train Index:\",train_idx,\",Val Index:\",valid_idx)\n",
    "\n",
    "    params = {\n",
    "    'nthread': 8, 'boosting_type': 'dart','objective': 'binary', 'metric': 'auc', 'learning_rate': 0.005, 'num_leaves': 70,\n",
    "    'max_depth': 7, 'subsample': 1, 'feature_fraction': 0.4, 'colsample_bytree': 0.08, 'min_split_gain': 0.09,\n",
    "    'min_child_weight': 9.5,\n",
    "    'verbose': 1, 'devic': 'gpu',\n",
    "    # parameters for dart\n",
    "    'drop_rate':0.7, 'skip_drop':0.7, 'max_drop':5, 'uniform_drop':False, 'xgboost_dart_mode':True, 'drop_seed':5 }\n",
    "\n",
    "    if n_fold >= 0:\n",
    "        dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "        dval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain)\n",
    "        bst = lgb.train(params, dtrain, num_boost_round=50000, valid_sets=[dval], early_stopping_rounds=5000, \n",
    "                        verbose_eval=5000)\n",
    "\n",
    "        tmp_valid = bst.predict(valid_x, num_iteration=bst.best_iteration)\n",
    "#         tmp_valid[valid_id>204000]=0\n",
    "        oof_preds[valid_idx] = tmp_valid\n",
    "        sub_preds += bst.predict(rx, num_iteration=bst.best_iteration) / folds.n_splits\n",
    "        \n",
    "#         sub_preds[rid>204000]=0\n",
    "\n",
    "        # Make the feature importance dataframe\n",
    "        gain = bst.feature_importance('gain')\n",
    "        fold_importance_df = pd.DataFrame({'feature':bst.feature_name(),'split':bst.feature_importance('split'),\n",
    "                                           'gain':100*gain/gain.sum(),  'fold':n_fold,}).sort_values('gain',ascending=False)\n",
    "\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "        del bst, train_x, train_y, valid_x, valid_y\n",
    "        \n",
    "xid=list(Xid)\n",
    "app_test = pd.read_csv('testing-set.csv', usecols=['order_id'])\n",
    "preds = pd.DataFrame({\"order_id\":app_test['order_id'], \"deal_or_not\":sub_preds})\n",
    "# create output sub-folder\n",
    "preds.to_csv(\"output/lgb_dart_\" + str(roc_auc_score(Y, oof_preds)) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
